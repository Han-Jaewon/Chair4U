{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609e5886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/ku/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¤ì •\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from time import time\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score, log_loss, ndcg_score\n",
    "from sklearn.model_selection import KFold\n",
    "from argparse import Namespace\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "DATA_PATH = './processed_data/'\n",
    "MODEL_SAVE_PATH = './saved_models/autoint/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2e2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤\n",
    "# ============================================================================\n",
    "\n",
    "def safe_divide(numerator, denominator, default=0.0):\n",
    "    \"\"\"ì•ˆì „í•œ ë‚˜ëˆ—ì…ˆ í—¬í¼ í•¨ìˆ˜\"\"\"\n",
    "    if denominator > 0:\n",
    "        return numerator / denominator\n",
    "    return default\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, chair_path, person_path, output_dir):\n",
    "        self.chair_path = chair_path\n",
    "        self.person_path = person_path\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # ì‚¬ìš©í•  Chair í”¼ì²˜ ì •ì˜\n",
    "        self.categorical_features = [\n",
    "            'í—¤ë“œë ˆìŠ¤íŠ¸ ìœ ë¬´', 'íŒ”ê±¸ì´ ìœ ë¬´', 'ìš”ì¶”ì§€ì§€ëŒ€ ìœ ë¬´', \n",
    "            'ë†’ì´ ì¡°ì ˆ ë ˆë²„ ìœ ë¬´', 'í‹¸íŒ… ì—¬ë¶€', 'ë“±ë°›ì´ ê³§/êº¾'\n",
    "        ]\n",
    "        \n",
    "        self.numerical_features = [\n",
    "            'h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MIN', 'h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MAX',\n",
    "            'b3_ì¢Œì„ ê°€ë¡œ ê¸¸ì´', 't4_ì¢Œì„ ì„¸ë¡œ ê¸¸ì´ ì¼ë°˜',\n",
    "            'b4_ë“±ë°›ì´ ê°€ë¡œ ê¸¸ì´', 'h7_ë“±ë°›ì´ ì„¸ë¡œ ê¸¸ì´'\n",
    "        ]\n",
    "        \n",
    "        self.person_features = [\n",
    "            'human-height', 'A_Buttock-popliteal length',\n",
    "            'B_Popliteal-height', 'C_Hip-breadth',\n",
    "            'F_Sitting-height', 'G_Shoulder-breadth'\n",
    "        ]\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬\"\"\"\n",
    "        # Chair ë°ì´í„° ë¡œë“œ\n",
    "        self.chair_df = pd.read_excel(self.chair_path, engine='openpyxl')\n",
    "        \n",
    "        # Person ë°ì´í„° ë¡œë“œ (cm -> mm ë³€í™˜)\n",
    "        self.person_df = pd.read_csv(self.person_path, encoding='utf-8')\n",
    "        for col in self.person_features:\n",
    "            if col in self.person_df.columns:\n",
    "                self.person_df[col] *= 10  # cm to mm\n",
    "        \n",
    "        # ë²”ì£¼í˜• í”¼ì²˜ ì „ì²˜ë¦¬\n",
    "        for col in self.categorical_features:\n",
    "            if col in self.chair_df.columns:\n",
    "                if col == 'ë“±ë°›ì´ ê³§/êº¾':\n",
    "                    self.chair_df[col] = self.chair_df[col].map({'ê³§': 0, 'êº¾': 1})\n",
    "                else:\n",
    "                    self.chair_df[col] = self.chair_df[col].map({'O': 1, 'X': 0})\n",
    "        \n",
    "        # ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "        self.chair_df['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MAX'] = np.where(pd.isna(self.chair_df['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MAX']),\n",
    "                                                        self.chair_df['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MIN'],\n",
    "                                                        self.chair_df['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MAX'])\n",
    "        self.chair_df[self.categorical_features] = self.chair_df[self.categorical_features].fillna(0)\n",
    "        self.chair_df[self.numerical_features] = self.chair_df[self.numerical_features].fillna(self.chair_df[self.numerical_features].mean())\n",
    "\n",
    "    def create_feature_mappings(self):\n",
    "        \"\"\"í”¼ì²˜ ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„±\"\"\"\n",
    "        self.feature_idx_map = {}\n",
    "        idx = 1\n",
    "        \n",
    "        # Person í”¼ì²˜ (6ê°œ)\n",
    "        for feat in self.person_features:\n",
    "            self.feature_idx_map[f'person_{feat}'] = idx\n",
    "            idx += 1\n",
    "        \n",
    "        # Chair ìˆ˜ì¹˜í˜• í”¼ì²˜ (6ê°œ)\n",
    "        for feat in self.numerical_features:\n",
    "            self.feature_idx_map[f'chair_{feat}'] = idx\n",
    "            idx += 1\n",
    "        \n",
    "        # ìƒí˜¸ì‘ìš© í”¼ì²˜ (6ê°œ)\n",
    "        interaction_features = [\n",
    "            'height_match_score', 'width_margin_ratio', 'depth_margin_ratio',\n",
    "            'backrest_height_ratio', 'shoulder_width_ratio', 'adjustable_range'\n",
    "        ]\n",
    "        for feat in interaction_features:\n",
    "            self.feature_idx_map[feat] = idx\n",
    "            idx += 1\n",
    "        \n",
    "        # ì´ì§„ ë²”ì£¼í˜• í”¼ì²˜ ì‹œì‘ ì˜¤í”„ì…‹\n",
    "        self.binary_offset = idx\n",
    "        \n",
    "        # ì´ì§„ í”¼ì²˜ ì¸ë±ìŠ¤\n",
    "        for i, feat in enumerate(self.categorical_features):\n",
    "            self.feature_idx_map[f'{feat}_0'] = self.binary_offset + i * 2\n",
    "            self.feature_idx_map[f'{feat}_1'] = self.binary_offset + i * 2 + 1\n",
    "\n",
    "    def calculate_interaction_features(self, person_row, chair_row):\n",
    "        \"\"\"Personê³¼ Chair ê°„ì˜ ìƒí˜¸ì‘ìš© í”¼ì²˜ ê³„ì‚°\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        h8_mid = (chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MIN'] + chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MAX']) / 2\n",
    "        h8_range = chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MAX'] - chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MIN']\n",
    "        popliteal_height = person_row['B_Popliteal-height']\n",
    "        \n",
    "        if h8_range > 0:\n",
    "            if chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MIN'] <= popliteal_height <= chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MAX']:\n",
    "                features['height_match_score'] = 1.0\n",
    "            else:\n",
    "                if popliteal_height < chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MIN']:\n",
    "                    dist = chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MIN'] - popliteal_height\n",
    "                else:\n",
    "                    dist = popliteal_height - chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MAX']\n",
    "                features['height_match_score'] = max(0, 1 - dist / 100)\n",
    "        else:\n",
    "            features['height_match_score'] = max(0, 1 - abs(h8_mid - popliteal_height) / 50)\n",
    "        \n",
    "        features['width_margin_ratio'] = safe_divide(\n",
    "            chair_row['b3_ì¢Œì„ ê°€ë¡œ ê¸¸ì´'] - person_row['C_Hip-breadth'], \n",
    "            person_row['C_Hip-breadth']\n",
    "        )\n",
    "        features['depth_margin_ratio'] = safe_divide(\n",
    "            person_row['A_Buttock-popliteal length'] - chair_row['t4_ì¢Œì„ ì„¸ë¡œ ê¸¸ì´ ì¼ë°˜'], \n",
    "            person_row['A_Buttock-popliteal length']\n",
    "        )\n",
    "        features['backrest_height_ratio'] = safe_divide(\n",
    "            chair_row['h7_ë“±ë°›ì´ ì„¸ë¡œ ê¸¸ì´'], \n",
    "            person_row['F_Sitting-height']\n",
    "        )\n",
    "        features['shoulder_width_ratio'] = safe_divide(\n",
    "            chair_row['b4_ë“±ë°›ì´ ê°€ë¡œ ê¸¸ì´'], \n",
    "            person_row['G_Shoulder-breadth']\n",
    "        )\n",
    "        features['adjustable_range'] = h8_range\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def check_matching_conditions(self, person_row, chair_row):\n",
    "        \"\"\"í•„ìˆ˜ ë§¤ì¹­ ì¡°ê±´ í™•ì¸ ë° ë ˆì´ë¸” ìƒì„±\"\"\"\n",
    "        conditions = {\n",
    "            't4 < A': chair_row['t4_ì¢Œì„ ì„¸ë¡œ ê¸¸ì´ ì¼ë°˜'] < person_row['A_Buttock-popliteal length'],\n",
    "            'h8 â‰ˆ B': (chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MIN'] <= person_row['B_Popliteal-height'] <= chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MAX']) \n",
    "                      if chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MAX'] > chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MIN']\n",
    "                      else abs((chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MIN'] + chair_row['h8_ì§€ë©´-ì¢Œì„ ë†’ì´_MAX'])/2 - person_row['B_Popliteal-height']) < 50,\n",
    "            'b3 > C': chair_row['b3_ì¢Œì„ ê°€ë¡œ ê¸¸ì´'] > person_row['C_Hip-breadth'],\n",
    "            'h7 < F': chair_row['h7_ë“±ë°›ì´ ì„¸ë¡œ ê¸¸ì´'] < person_row['F_Sitting-height'],\n",
    "            'b4 â‰¥ G': chair_row['b4_ë“±ë°›ì´ ê°€ë¡œ ê¸¸ì´'] >= person_row['G_Shoulder-breadth']\n",
    "        }\n",
    "        \n",
    "        all_satisfied = all(conditions.values())\n",
    "        soft_label = sum(conditions.values()) / len(conditions)\n",
    "        \n",
    "        return int(all_satisfied), soft_label, conditions\n",
    "    \n",
    "    def process_data(self):\n",
    "        \"\"\"ì „ì²´ ë°ì´í„° ì²˜ë¦¬ ë° íŒŒì¼ ìƒì„±\"\"\"\n",
    "        self.load_data()\n",
    "        self.create_feature_mappings()\n",
    "        \n",
    "        f_train_value = open(os.path.join(self.output_dir, 'train_x.txt'), 'w')\n",
    "        f_train_index = open(os.path.join(self.output_dir, 'train_i.txt'), 'w')\n",
    "        f_train_label = open(os.path.join(self.output_dir, 'train_y.txt'), 'w')\n",
    "        \n",
    "        cnt = 0\n",
    "        positive_cnt = 0\n",
    "        \n",
    "        for _, person in self.person_df.iterrows():\n",
    "            for _, chair in self.chair_df.iterrows():\n",
    "                cnt += 1\n",
    "                \n",
    "                values = []\n",
    "                indices = []\n",
    "                \n",
    "                # Person ìˆ˜ì¹˜í˜• í”¼ì²˜\n",
    "                for feat in self.person_features:\n",
    "                    if feat in person.index:\n",
    "                        values.append(str(person[feat]))\n",
    "                        indices.append(str(self.feature_idx_map[f'person_{feat}']))\n",
    "                \n",
    "                # Chair ìˆ˜ì¹˜í˜• í”¼ì²˜\n",
    "                for feat in self.numerical_features:\n",
    "                    values.append(str(chair[feat]))\n",
    "                    indices.append(str(self.feature_idx_map[f'chair_{feat}']))\n",
    "                \n",
    "                # ìƒí˜¸ì‘ìš© í”¼ì²˜\n",
    "                interaction_feats = self.calculate_interaction_features(person, chair)\n",
    "                for feat_name, feat_value in interaction_feats.items():\n",
    "                    values.append(str(feat_value))\n",
    "                    indices.append(str(self.feature_idx_map[feat_name]))\n",
    "                \n",
    "                # ì´ì§„ ë²”ì£¼í˜• í”¼ì²˜\n",
    "                for feat in self.categorical_features:\n",
    "                    values.append('1')\n",
    "                    feat_value = int(chair[feat]) if not pd.isna(chair[feat]) else 0\n",
    "                    idx_key = f'{feat}_{feat_value}'\n",
    "                    indices.append(str(self.feature_idx_map[idx_key]))\n",
    "                \n",
    "                # ë ˆì´ë¸” ê³„ì‚°\n",
    "                hard_label, soft_label, conditions = self.check_matching_conditions(person, chair)\n",
    "                \n",
    "                f_train_value.write(' '.join(values) + '\\n')\n",
    "                f_train_index.write(' '.join(indices) + '\\n')\n",
    "                f_train_label.write(f'{soft_label:.4f}\\n')\n",
    "                \n",
    "                if hard_label == 1:\n",
    "                    positive_cnt += 1\n",
    "                \n",
    "                if cnt % 100 == 0:\n",
    "                    print(f'Processed {cnt} combinations...')\n",
    "        \n",
    "        f_train_value.close()\n",
    "        f_train_index.close()\n",
    "        f_train_label.close()\n",
    "        \n",
    "        print(f\"\\nTotal combinations: {cnt}\")\n",
    "        print(f\"Positive matches: {positive_cnt} ({positive_cnt/cnt*100:.2f}%)\")\n",
    "        \n",
    "        # ë©”íƒ€ë°ì´í„° ì €ì¥\n",
    "        metadata = {\n",
    "            'feature_mappings': self.feature_idx_map,\n",
    "            'total_features': len(self.feature_idx_map),\n",
    "            'numerical_features': self.binary_offset - 1,\n",
    "            'categorical_features': self.categorical_features,\n",
    "            'person_features': self.person_features,\n",
    "            'chair_numerical_features': self.numerical_features\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, 'metadata.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4605b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. K-fold ë¶„í• \n",
    "# ============================================================================\n",
    "\n",
    "def _load_data(_nrows=None):\n",
    "    \"\"\"train_x.txtì™€ train_y.txt íŒŒì¼ì„ ì½ì–´ ë°ì´í„°ë¥¼ ë¡œë“œ\"\"\"\n",
    "    train_x = pd.read_csv(DATA_PATH + 'train_x.txt', header=None, sep=' ', nrows=_nrows, dtype=np.float64)\n",
    "    train_y = pd.read_csv(DATA_PATH + 'train_y.txt', header=None, sep=' ', nrows=_nrows, dtype=np.float64)\n",
    "    \n",
    "    train_x = train_x.values\n",
    "    train_y = train_y.values.reshape([-1])\n",
    "    \n",
    "    print('Data loading done!')\n",
    "    print('Training data: %d' % train_y.shape[0])\n",
    "    \n",
    "    assert train_x.shape[0] == train_y.shape[0]\n",
    "    return train_x, train_y\n",
    "\n",
    "def save_x_y(fold_index, train_x, train_y):\n",
    "    \"\"\"10ê°œì˜ foldë¡œ ë¶„í• ëœ ë°ì´í„°ë¥¼ ê°ê° ì €ì¥\"\"\"\n",
    "    _get = lambda x, l: [x[i] for i in l]\n",
    "\n",
    "    for i in range(len(fold_index)):\n",
    "        print(\"Now part %d\" % (i+1))\n",
    "        part_index = fold_index[i]\n",
    "        \n",
    "        Xv_train_, y_train_ = _get(train_x, part_index), _get(train_y, part_index)\n",
    "        \n",
    "        save_dir = DATA_PATH + \"part\" + str(i+1) + \"/\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "        np.save(save_dir + 'train_x.npy', Xv_train_)\n",
    "        np.save(save_dir + 'train_y.npy', y_train_)\n",
    "\n",
    "def save_i(fold_index):\n",
    "    \"\"\"train_i.txt íŒŒì¼ì„ ì½ì–´ ë²”ì£¼í˜• íŠ¹ì„± ì¸ë±ìŠ¤ë¥¼ foldë³„ë¡œ ì €ì¥\"\"\"\n",
    "    _get = lambda x, l: [x[i] for i in l]\n",
    "    \n",
    "    train_i = pd.read_csv(DATA_PATH + 'train_i.txt', header=None, sep=' ', dtype=np.float64)\n",
    "    train_i = train_i.values\n",
    "    \n",
    "    feature_size = train_i.max() + 1\n",
    "    print(\"Feature size = %d\" % feature_size)\n",
    "    \n",
    "    np.save(DATA_PATH + \"feature_size.npy\", np.array([feature_size]))\n",
    "    \n",
    "    for i in range(len(fold_index)):\n",
    "        print(\"Now part %d\" % (i+1))\n",
    "        part_index = fold_index[i]\n",
    "        Xi_train_ = _get(train_i, part_index)\n",
    "        np.save(DATA_PATH + \"part\" + str(i+1) + '/train_i.npy', Xi_train_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1b3e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. ìŠ¤ì¼€ì¼ë§\n",
    "# ============================================================================\n",
    "\n",
    "def scale(x):\n",
    "    \"\"\"ê°œë³„ ìˆ˜ì¹˜í˜• í”¼ì²˜ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    if x > 2:\n",
    "        x = int(math.log(float(x))**2)\n",
    "    return x\n",
    "\n",
    "def scale_each_fold():\n",
    "    \"\"\"10ê°œì˜ foldì— ëŒ€í•´ ê°ê° ìŠ¤ì¼€ì¼ë§ì„ ìˆ˜í–‰\"\"\"\n",
    "    for i in range(1, 10): # ìˆ˜ì •ì „: 11\n",
    "        print('Now part %d' % i)\n",
    "        \n",
    "        data = np.load(DATA_PATH + 'part'+str(i)+'/train_x.npy')\n",
    "        part = data[:, 0:18]  # ì²˜ìŒ 18ê°œ ìˆ˜ì¹˜í˜• í”¼ì²˜ë§Œ\n",
    "        \n",
    "        for j in range(part.shape[0]):\n",
    "            if j % 100 == 0:\n",
    "                print(j)\n",
    "            part[j] = list(map(scale, part[j]))\n",
    "        \n",
    "        np.save(DATA_PATH + 'part' + str(i) + '/train_x2.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8578613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. DRM (Differentiable Ranking Metrics) êµ¬í˜„\n",
    "# ============================================================================\n",
    "\n",
    "class DifferentiableRankingLoss:\n",
    "    \"\"\"TensorFlow implementation of differentiable ranking metrics for top-k\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def detNeuralSort(s, tau=1.0, k=5):\n",
    "        \"\"\"Deterministic neural sort for ranking\"\"\"\n",
    "        batch_size = tf.shape(s)[0]\n",
    "        n = tf.shape(s)[1]\n",
    "        \n",
    "        # Expand dimensions for broadcasting\n",
    "        su = tf.expand_dims(s, axis=-1)  # [batch_size, n_items, 1]\n",
    "        \n",
    "        # Create matrices\n",
    "        one = tf.ones([n, 1], dtype=tf.float32)\n",
    "        one_k = tf.ones([1, k], dtype=tf.float32)\n",
    "        \n",
    "        # Compute A_s = |s_i - s_j|\n",
    "        A_s = tf.abs(su - tf.transpose(su, [0, 2, 1]))  # [batch_size, n, n]\n",
    "        \n",
    "        # Compute B\n",
    "        B = tf.matmul(A_s, tf.matmul(one, one_k))  # [batch_size, n, k]\n",
    "        \n",
    "        # Compute scaling\n",
    "        scaling = tf.cast(n + 1 - 2 * (tf.range(n) + 1), tf.float32)\n",
    "        scaling = tf.expand_dims(scaling, 0)  # [1, n]\n",
    "        \n",
    "        # Compute C\n",
    "        C = tf.expand_dims(s * scaling, -1)[:, :, :k]  # [batch_size, n, k]\n",
    "        \n",
    "        # Compute P_max\n",
    "        P_max = tf.transpose(C - B, [0, 2, 1])  # [batch_size, k, n]\n",
    "        \n",
    "        # Apply softmax\n",
    "        P_hat = tf.nn.softmax(P_max / tau, axis=-1)\n",
    "        \n",
    "        return P_hat\n",
    "    \n",
    "    @staticmethod\n",
    "    def neuNDCGLoss(scores, labels, k=5, tau=10.0):\n",
    "        \"\"\"Neural NDCG Loss for ranking\"\"\"\n",
    "        batch_size = tf.shape(scores)[0]\n",
    "        n_items = tf.shape(scores)[1]\n",
    "        \n",
    "        # Create discount matrix\n",
    "        discounts = 1.0 / tf.math.log(tf.range(2, k + 2, dtype=tf.float32))  # log base is e, so we use range(2, k+2)\n",
    "        diag = tf.linalg.diag(discounts)\n",
    "        \n",
    "        # Get top-k for efficiency (ì‚¬ì‹¤ìƒ ì „ì²´ë¥¼ ë‹¤ ì‚¬ìš©í•˜ì§€ë§Œ êµ¬ì¡°ìƒ í•„ìš”)\n",
    "        k_actual = tf.minimum(k, n_items)\n",
    "        \n",
    "        # Neural sort ì ìš©\n",
    "        P_hat = DifferentiableRankingLoss.detNeuralSort(scores, tau=tau, k=k_actual)\n",
    "        \n",
    "        # IDCG ê³„ì‚° (Ideal DCG)\n",
    "        sorted_labels, _ = tf.nn.top_k(labels, k=k_actual)\n",
    "        ideal_dcg = tf.reduce_sum(sorted_labels * discounts[:k_actual], axis=1)\n",
    "        \n",
    "        # DCG ê³„ì‚°\n",
    "        # P_hat: [batch_size, k, n_items], labels: [batch_size, n_items]\n",
    "        # ê° í¬ì§€ì…˜ì—ì„œì˜ ê¸°ëŒ€ ë ˆì´ë¸” ê°’ ê³„ì‚°\n",
    "        expected_labels_at_positions = tf.matmul(P_hat, tf.expand_dims(labels, -1))  # [batch_size, k, 1]\n",
    "        expected_labels_at_positions = tf.squeeze(expected_labels_at_positions, -1)  # [batch_size, k]\n",
    "        \n",
    "        # DCG ê³„ì‚°\n",
    "        dcg = tf.reduce_sum(expected_labels_at_positions * discounts[:k_actual], axis=1)\n",
    "        \n",
    "        # NDCG ê³„ì‚° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)\n",
    "        ndcg = dcg / (ideal_dcg + 1e-10)\n",
    "        \n",
    "        # LossëŠ” -NDCG (ìµœëŒ€í™”í•˜ê¸° ìœ„í•´)\n",
    "        loss = -tf.reduce_mean(ndcg)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def neuPrecisionLoss(scores, labels, k=5, tau=10.0):\n",
    "        \"\"\"Neural Precision@k Loss\"\"\"\n",
    "        batch_size = tf.shape(scores)[0]\n",
    "        n_items = tf.shape(scores)[1]\n",
    "        k_actual = tf.minimum(k, n_items)\n",
    "        \n",
    "        # Neural sort\n",
    "        P_hat = DifferentiableRankingLoss.detNeuralSort(scores, tau=tau, k=k_actual)\n",
    "        \n",
    "        # Precision@k ê³„ì‚°\n",
    "        # P_hat: [batch_size, k, n_items]ì—ì„œ ê° í¬ì§€ì…˜ì˜ ê¸°ëŒ€ ë ˆì´ë¸” ê°’\n",
    "        expected_labels = tf.matmul(P_hat, tf.expand_dims(labels, -1))  # [batch_size, k, 1]\n",
    "        expected_labels = tf.squeeze(expected_labels, -1)  # [batch_size, k]\n",
    "        \n",
    "        # Precision = (top-kì—ì„œ relevantí•œ ê²ƒë“¤ì˜ ìˆ˜) / k\n",
    "        precision = tf.reduce_sum(expected_labels, axis=1) / tf.cast(k_actual, tf.float32)\n",
    "        \n",
    "        # LossëŠ” -Precision\n",
    "        loss = -tf.reduce_mean(precision)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c46e734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. AutoInt ëª¨ë¸ ì •ì˜ (Ranking Loss í¬í•¨)\n",
    "# ============================================================================\n",
    "\n",
    "def normalize(inputs, epsilon=1e-8):\n",
    "    \"\"\"Layer Normalization\"\"\"\n",
    "    inputs_shape = inputs.get_shape()\n",
    "    params_shape = inputs_shape[-1:]\n",
    "\n",
    "    mean, variance = tf.nn.moments(inputs, [-1], keepdims=True)\n",
    "    beta = tf.Variable(tf.zeros(params_shape))\n",
    "    gamma = tf.Variable(tf.ones(params_shape))\n",
    "    normalized = (inputs - mean) / ((variance + epsilon) ** (.5))\n",
    "    outputs = gamma * normalized + beta\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def multihead_attention(queries, keys, values, num_units=None, num_heads=1,\n",
    "                        dropout_keep_prob=1, is_training=True, has_residual=True):\n",
    "    \"\"\"Multi-head Self-Attention\"\"\"\n",
    "    if num_units is None:\n",
    "        num_units = queries.get_shape().as_list()[-1]\n",
    "\n",
    "    # Linear projections\n",
    "    Q = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(queries)\n",
    "    K = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(keys)\n",
    "    V = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(values)\n",
    "    if has_residual:\n",
    "        V_res = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(values)\n",
    "\n",
    "    # Split and concat\n",
    "    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)\n",
    "    K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)\n",
    "    V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)\n",
    "\n",
    "    # Multiplication\n",
    "    weights = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))\n",
    "    weights = weights / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "    weights = tf.nn.softmax(weights)\n",
    "\n",
    "    # Dropouts\n",
    "    weights = tf.cond(is_training,\n",
    "                      lambda: tf.nn.dropout(weights, keep_prob=dropout_keep_prob),\n",
    "                      lambda: weights)\n",
    "\n",
    "    # Weighted sum\n",
    "    outputs = tf.matmul(weights, V_)\n",
    "    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)\n",
    "\n",
    "    # Residual connection\n",
    "    if has_residual:\n",
    "        outputs += V_res\n",
    "\n",
    "    outputs = tf.nn.relu(outputs)\n",
    "    outputs = normalize(outputs)\n",
    "        \n",
    "    return outputs\n",
    "\n",
    "class AutoInt():\n",
    "    def __init__(self, args, feature_size, run_cnt):\n",
    "        \"\"\"AutoInt ëª¨ë¸ ì´ˆê¸°í™”\"\"\"\n",
    "        self.feature_size = int(feature_size)\n",
    "        self.field_size = args.field_size\n",
    "        self.run_cnt = run_cnt\n",
    "        self.embedding_size = int(args.embedding_size)\n",
    "        self.blocks = args.blocks\n",
    "        self.heads = args.heads\n",
    "        self.block_shape = args.block_shape\n",
    "        self.output_size = args.block_shape[-1]\n",
    "        self.has_residual = args.has_residual\n",
    "        self.deep_layers = args.deep_layers\n",
    "\n",
    "        # í•™ìŠµ ê´€ë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "        self.batch_norm = args.batch_norm\n",
    "        self.batch_norm_decay = args.batch_norm_decay\n",
    "        self.drop_keep_prob = args.dropout_keep_prob\n",
    "        self.l2_reg = args.l2_reg\n",
    "        self.epoch = args.epoch\n",
    "        self.batch_size = args.batch_size\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.optimizer_type = args.optimizer_type\n",
    "\n",
    "        # ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "        self.save_path = args.save_path + str(run_cnt) + '/'\n",
    "        self.is_save = args.is_save\n",
    "        if (args.is_save == True and os.path.exists(self.save_path) == False):\n",
    "            os.makedirs(self.save_path)\t\n",
    "\n",
    "        self.verbose = args.verbose\n",
    "        self.random_seed = args.random_seed\n",
    "        # Ranking loss ê´€ë ¨ íŒŒë¼ë¯¸í„°\n",
    "        self.loss_type = args.loss_type\n",
    "        self.ranking_k = getattr(args, 'ranking_k', 5)\n",
    "        self.ranking_tau = getattr(args, 'ranking_tau', 10.0)\n",
    "        self.eval_metric = roc_auc_score\n",
    "        self.best_loss = 1.0\n",
    "        self.greater_is_better = args.greater_is_better\n",
    "        self.train_result, self.valid_result = [], []\n",
    "        self.train_loss, self.valid_loss = [], []\n",
    "        \n",
    "        self._init_graph()\n",
    "\n",
    "    def _init_graph(self):\n",
    "        \"\"\"TensorFlow ê³„ì‚° ê·¸ë˜í”„ ì´ˆê¸°í™”\"\"\"\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "\n",
    "            # ì…ë ¥ placeholder ì •ì˜\n",
    "            self.feat_index = tf.placeholder(tf.int32, shape=[None, None], name=\"feat_index\")\n",
    "            self.feat_value = tf.placeholder(tf.float32, shape=[None, None], name=\"feat_value\")\n",
    "            self.label = tf.placeholder(tf.float32, shape=[None, 1], name=\"label\")\n",
    "            self.dropout_keep_prob = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_prob\")\n",
    "            self.train_phase = tf.placeholder(tf.bool, name=\"train_phase\")\n",
    "\n",
    "            self.weights = self._initialize_weights()\n",
    "\n",
    "            # 1. Embedding layer\n",
    "            self.embeddings = tf.nn.embedding_lookup(self.weights[\"feature_embeddings\"], self.feat_index)\n",
    "            feat_value = tf.reshape(self.feat_value, shape=[-1, tf.shape(self.feat_index)[1], 1])\n",
    "            self.embeddings = tf.multiply(self.embeddings, feat_value)\n",
    "            self.embeddings = tf.nn.dropout(self.embeddings, self.dropout_keep_prob[1])\n",
    "            \n",
    "            # 2. DNN ë¶€ë¶„ (ì„ íƒì )\n",
    "            if self.deep_layers != None:\n",
    "                self.y_dense = tf.reshape(self.embeddings, shape=[-1, self.field_size * self.embedding_size])\n",
    "                \n",
    "                for i in range(0, len(self.deep_layers)):\n",
    "                    self.y_dense = tf.add(tf.matmul(self.y_dense, self.weights[\"layer_%d\" %i]), self.weights[\"bias_%d\"%i])\n",
    "                    if self.batch_norm:\n",
    "                        bn_layer = BatchNormalization(momentum=self.batch_norm_decay, epsilon=1e-5, center=True, scale=True, name=\"bn_%d\" % i)\n",
    "                        self.y_dense = bn_layer(self.y_dense, training=self.train_phase)\n",
    "                    self.y_dense = tf.nn.relu(self.y_dense)\n",
    "                    self.y_dense = tf.nn.dropout(self.y_dense, self.dropout_keep_prob[2])\n",
    "                    \n",
    "                self.y_dense = tf.add(tf.matmul(self.y_dense, self.weights[\"prediction_dense\"]),\n",
    "                                      self.weights[\"prediction_bias_dense\"], name='logits_dense')\n",
    "            \n",
    "            # 3. AutoInt í•µì‹¬ ë¶€ë¶„: Multi-head Self-Attention\n",
    "            self.y_deep = self.embeddings\n",
    "            for i in range(self.blocks):   \n",
    "                self.y_deep = multihead_attention(queries=self.y_deep, keys=self.y_deep, values=self.y_deep,\n",
    "                                                  num_units=self.block_shape[i], num_heads=self.heads,\n",
    "                                                  dropout_keep_prob=self.dropout_keep_prob[0],\n",
    "                                                  is_training=self.train_phase, has_residual=self.has_residual)\n",
    "\n",
    "            # Flatten\n",
    "            self.flat = tf.reshape(self.y_deep, [tf.shape(self.y_deep)[0], -1])\n",
    "\n",
    "            # ìµœì¢… ì˜ˆì¸¡\n",
    "            self.y = tf.add(tf.matmul(self.flat, self.weights['prediction']), self.weights['prediction_bias'], name='logits')\n",
    "            self.out = tf.nn.sigmoid(self.y)\n",
    "\n",
    "            # DNNê³¼ AutoInt ê²°í•© (ì„ íƒì )\n",
    "            if self.deep_layers != None:\n",
    "                self.out = tf.nn.sigmoid(self.y + self.y_dense)\n",
    "            else:\n",
    "                self.out = tf.nn.sigmoid(self.y)\n",
    "        \n",
    "            # ============ Ranking ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ ============\n",
    "            if self.loss_type == \"ranking_ndcg\":\n",
    "                # ë°°ì¹˜ë¥¼ ì¬êµ¬ì„±: ê° ì‚¬ëŒë‹¹ ì—¬ëŸ¬ ì˜ìë“¤ì„ í•˜ë‚˜ì˜ ë­í‚¹ ë¬¸ì œë¡œ ì²˜ë¦¬\n",
    "                # í˜„ì¬ êµ¬ì¡°ì—ì„œëŠ” ë°°ì¹˜ ë‚´ì˜ ëª¨ë“  ì•„ì´í…œì„ í•˜ë‚˜ì˜ ë­í‚¹ìœ¼ë¡œ ì²˜ë¦¬\n",
    "                self.loss = DifferentiableRankingLoss.neuNDCGLoss(\n",
    "                    scores=self.out, \n",
    "                    labels=self.label, \n",
    "                    k=self.ranking_k,\n",
    "                    tau=self.ranking_tau\n",
    "                )\n",
    "            elif self.loss_type == \"ranking_precision\":\n",
    "                self.loss = DifferentiableRankingLoss.neuPrecisionLoss(\n",
    "                    scores=self.out,\n",
    "                    labels=self.label,\n",
    "                    k=self.ranking_k, \n",
    "                    tau=self.ranking_tau\n",
    "                )\n",
    "            elif self.loss_type == \"logloss\":\n",
    "                # ê¸°ë³¸ logloss (ë¹„êµìš©)\n",
    "                self.loss = tf.losses.log_loss(self.label, self.out)\n",
    "            elif self.loss_type == \"mse\":\n",
    "                self.loss = tf.nn.l2_loss(tf.subtract(self.label, self.out))\n",
    "            else:\n",
    "                # ê¸°ë³¸ê°’: soft ranking loss (weighted binary cross entropy)\n",
    "                epsilon = 1e-7\n",
    "                self.loss = tf.reduce_mean(\n",
    "                    -self.label * tf.log(self.out + epsilon) - \n",
    "                    (1 - self.label) * tf.log(1 - self.out + epsilon)\n",
    "                )\n",
    "\n",
    "            # L2 ì •ê·œí™”\n",
    "            if self.l2_reg > 0:\n",
    "                reg_weights = [weight for name, weight in self.weights.items() if 'bias' not in name]\n",
    "                if reg_weights:\n",
    "                    self.loss += tf.add_n([tf.nn.l2_loss(w) for w in reg_weights]) * self.l2_reg\n",
    "           \n",
    "            # Optimizer ì„¤ì •\n",
    "            self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            \n",
    "            if self.optimizer_type == \"adam\":\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(self.loss, global_step=self.global_step)\n",
    "            elif self.optimizer_type == \"adagrad\":\n",
    "                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate, initial_accumulator_value=1e-8).minimize(self.loss, global_step=self.global_step)\n",
    "            elif self.optimizer_type == \"gd\":\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "            # ì´ˆê¸°í™”\n",
    "            self.saver = tf.train.Saver(max_to_keep=5)\n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess = self._init_session()\n",
    "            self.sess.run(init)\n",
    "            self.count_param()\n",
    "\n",
    "    def count_param(self):\n",
    "        \"\"\"ëª¨ë¸ì˜ ì „ì²´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚°\"\"\"\n",
    "        k = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "        print(\"Total parameters: %d\" % k) \n",
    "        print(\"Extra parameters: %d\" % (k - self.feature_size * self.embedding_size))\n",
    "\n",
    "    def _init_session(self):\n",
    "        \"\"\"TensorFlow ì„¸ì…˜ ì´ˆê¸°í™”\"\"\"\n",
    "        config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        config.gpu_options.allow_growth = True\n",
    "        return tf.Session(config=config)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"ëª¨ë¸ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\"\"\"\n",
    "        weights = dict()\n",
    "\n",
    "        # íŠ¹ì„± ì„ë² ë”© í…Œì´ë¸”\n",
    "        weights[\"feature_embeddings\"] = tf.Variable(\n",
    "            tf.random_normal([self.feature_size, self.embedding_size], 0.0, 0.01),\n",
    "            name=\"feature_embeddings\")\n",
    "\n",
    "        if self.blocks > 0:\n",
    "            final_attention_size = self.block_shape[-1] * self.field_size\n",
    "            input_size = final_attention_size\n",
    "        else:\n",
    "            input_size = self.embedding_size * self.field_size\n",
    "\n",
    "        # DNN ë ˆì´ì–´ ê°€ì¤‘ì¹˜ (ì„ íƒì )\n",
    "        if self.deep_layers != None:\n",
    "            num_layer = len(self.deep_layers)\n",
    "            layer0_size = self.field_size * self.embedding_size\n",
    "            glorot = np.sqrt(2.0 / (layer0_size + self.deep_layers[0]))\n",
    "            weights[\"layer_0\"] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(layer0_size, self.deep_layers[0])), dtype=np.float32)\n",
    "            weights[\"bias_0\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[0])), dtype=np.float32)\n",
    "            \n",
    "            for i in range(1, num_layer):\n",
    "                glorot = np.sqrt(2.0 / (self.deep_layers[i-1] + self.deep_layers[i]))\n",
    "                weights[\"layer_%d\" % i] = tf.Variable(\n",
    "                    np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i-1], self.deep_layers[i])), dtype=np.float32)\n",
    "                weights[\"bias_%d\" % i] = tf.Variable(\n",
    "                    np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])), dtype=np.float32)\n",
    "            \n",
    "            glorot = np.sqrt(2.0 / (self.deep_layers[-1] + 1))\n",
    "            weights[\"prediction_dense\"] = tf.Variable(\n",
    "                                np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[-1], 1)), dtype=np.float32, name=\"prediction_dense\")\n",
    "            weights[\"prediction_bias_dense\"] = tf.Variable(np.random.normal(), dtype=np.float32, name=\"prediction_bias_dense\")\n",
    "\n",
    "        # AutoInt ìµœì¢… ì˜ˆì¸¡ì¸µ ê°€ì¤‘ì¹˜\n",
    "        glorot = np.sqrt(2.0 / (input_size + 1))\n",
    "        weights[\"prediction\"] = tf.Variable(\n",
    "                            np.random.normal(loc=0, scale=glorot, size=(input_size, 1)), dtype=np.float32, name=\"prediction\")\n",
    "        weights[\"prediction_bias\"] = tf.Variable(np.random.normal(), dtype=np.float32, name=\"prediction_bias\")\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def get_batch(self, Xi, Xv, y, batch_size, index):\n",
    "        \"\"\"ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ\"\"\"\n",
    "        start = index * batch_size\n",
    "        end = (index+1) * batch_size\n",
    "        end = end if end < len(y) else len(y)\n",
    "        return Xi[start:end], Xv[start:end], [[y_] for y_ in y[start:end]]\n",
    "\n",
    "    def shuffle_in_unison_scary(self, a, b, c):\n",
    "        \"\"\"ì„¸ ê°œì˜ ë°°ì—´ì„ ë™ì¼í•œ ìˆœì„œë¡œ ì…”í”Œ\"\"\"\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(a)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(b)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(c)\n",
    "\n",
    "    def fit_on_batch(self, Xi, Xv, y):\n",
    "        \"\"\"í•œ ë°°ì¹˜ì— ëŒ€í•œ í•™ìŠµ ìˆ˜í–‰\"\"\"\n",
    "        feed_dict = {self.feat_index: Xi, self.feat_value: Xv, self.label: y,\n",
    "                     self.dropout_keep_prob: self.drop_keep_prob, self.train_phase: True}\n",
    "        step, loss, opt = self.sess.run((self.global_step, self.loss, self.optimizer), feed_dict=feed_dict)\n",
    "        return step, loss\n",
    "\n",
    "    def fit_once(self, Xi_train, Xv_train, y_train, epoch, file_count, Xi_valid=None, Xv_valid=None, y_valid=None, early_stopping=False):\n",
    "        \"\"\"í•˜ë‚˜ì˜ ë°ì´í„° íŒŒì¼ì— ëŒ€í•œ ì „ì²´ í•™ìŠµ ìˆ˜í–‰\"\"\"\n",
    "        has_valid = Xv_valid is not None\n",
    "        last_step = 0\n",
    "        t1 = time()\n",
    "        \n",
    "        self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
    "        total_batch = int(len(y_train) / self.batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train, self.batch_size, i)\n",
    "            step, loss = self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
    "            last_step = step\n",
    "\n",
    "        # í•™ìŠµ ë°ì´í„° í‰ê°€\n",
    "        train_result, train_loss = self.evaluate(Xi_train, Xv_train, y_train)\n",
    "        self.train_result.append(train_result)\n",
    "        self.train_loss.append(train_loss)\n",
    "\n",
    "        # ê²€ì¦ ë°ì´í„° í‰ê°€\n",
    "        if has_valid:\n",
    "            valid_result, valid_loss = self.evaluate(Xi_valid, Xv_valid, y_valid)\n",
    "            self.valid_result.append(valid_result)\n",
    "            self.valid_loss.append(valid_loss)\n",
    "\n",
    "            # ìµœì  ëª¨ë¸ ì €ì¥\n",
    "            if valid_loss < self.best_loss and self.is_save == True:\n",
    "                old_loss = self.best_loss\n",
    "                self.best_loss = valid_loss\n",
    "                self.saver.save(self.sess, self.save_path + 'model.ckpt', global_step=last_step)\n",
    "                print(\"[%d-%d] Model saved! Valid loss improved from %.4f to %.4f\" % (epoch, file_count, old_loss, self.best_loss))\n",
    "\n",
    "        # í•™ìŠµ ê²°ê³¼ ì¶œë ¥\n",
    "        if self.verbose > 0 and ((epoch-1)*9 + file_count) % self.verbose == 0:\n",
    "            if has_valid:\n",
    "                print(\"[%d-%d] train-result=%.4f, train-loss=%.4f, valid-result=%.4f, valid-loss=%.4f [%.1f s]\" % \n",
    "                      (epoch, file_count, train_result, train_loss, valid_result, valid_loss, time() - t1))\n",
    "            else:\n",
    "                print(\"[%d-%d] train-result=%.4f [%.1f s]\" % (epoch, file_count, train_result, time() - t1))\n",
    "                \n",
    "        return True\n",
    "\n",
    "    def predict(self, Xi, Xv):\n",
    "        \"\"\"ì˜ˆì¸¡ ìˆ˜í–‰\"\"\"\n",
    "        dummy_y = [1] * len(Xi)\n",
    "        batch_index = 0\n",
    "        Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
    "        y_pred = None\n",
    "\n",
    "        while len(Xi_batch) > 0:\n",
    "            num_batch = len(y_batch)\n",
    "            feed_dict = {self.feat_index: Xi_batch, self.feat_value: Xv_batch, self.label: y_batch,\n",
    "                         self.dropout_keep_prob: [1.0] * len(self.drop_keep_prob), self.train_phase: False}\n",
    "            batch_out = self.sess.run(self.out, feed_dict=feed_dict)\n",
    "\n",
    "            if batch_index == 0:\n",
    "                y_pred = np.reshape(batch_out, (num_batch,))\n",
    "            else:\n",
    "                y_pred = np.concatenate((y_pred, np.reshape(batch_out, (num_batch,))))\n",
    "\n",
    "            batch_index += 1\n",
    "            Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, Xi, Xv, y):\n",
    "        \"\"\"ëª¨ë¸ í‰ê°€ - Top-k ì§€í‘œ ì¤‘ì‹¬\"\"\"\n",
    "        y_pred = self.predict(Xi, Xv)\n",
    "        y_pred = np.clip(y_pred, 1e-6, 1-1e-6)\n",
    "\n",
    "        # Top-k í‰ê°€ ì§€í‘œë“¤\n",
    "        results = {}\n",
    "        \n",
    "        # NDCG@5, NDCG@10 ê³„ì‚°\n",
    "        for k in [5, 10, 20]:\n",
    "            try:\n",
    "                if len(y) >= k:\n",
    "                    ndcg_k = ndcg_score(y.reshape(1, -1), y_pred.reshape(1, -1), k=k)\n",
    "                    results[f'ndcg@{k}'] = ndcg_k\n",
    "                else:\n",
    "                    results[f'ndcg@{k}'] = 0.0\n",
    "            except:\n",
    "                results[f'ndcg@{k}'] = 0.0\n",
    "        \n",
    "        # Precision@k ê³„ì‚°\n",
    "        for k in [5, 10, 20]:\n",
    "            if len(y) >= k:\n",
    "                # Top-k ì˜ˆì¸¡ì˜ ì¸ë±ìŠ¤\n",
    "                top_k_indices = np.argsort(y_pred)[-k:]\n",
    "                # Top-kì—ì„œ relevant items ìˆ˜ (soft label > 0.5)\n",
    "                relevant_in_topk = np.sum(y[top_k_indices] > 0.5)\n",
    "                precision_k = relevant_in_topk / k\n",
    "                results[f'precision@{k}'] = precision_k\n",
    "            else:\n",
    "                results[f'precision@{k}'] = 0.0\n",
    "        \n",
    "        # Recall@k ê³„ì‚°\n",
    "        total_relevant = np.sum(y > 0.5)\n",
    "        for k in [5, 10, 20]:\n",
    "            if len(y) >= k and total_relevant > 0:\n",
    "                top_k_indices = np.argsort(y_pred)[-k:]\n",
    "                relevant_in_topk = np.sum(y[top_k_indices] > 0.5)\n",
    "                recall_k = relevant_in_topk / total_relevant\n",
    "                results[f'recall@{k}'] = recall_k\n",
    "            else:\n",
    "                results[f'recall@{k}'] = 0.0\n",
    "        \n",
    "        # ë©”ì¸ ë©”íŠ¸ë¦­ìœ¼ë¡œ NDCG@10 ì‚¬ìš©\n",
    "        main_metric = results.get('ndcg@10', 0.0)\n",
    "        \n",
    "        # ì†ì‹¤ ê³„ì‚° (ranking loss ê·¼ì‚¬)\n",
    "        epsilon = 1e-7\n",
    "        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.mean(y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped))\n",
    "        \n",
    "        # ìƒì„¸ ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"Evaluation Results:\")\n",
    "        for metric, value in results.items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "        return main_metric, loss\n",
    "\n",
    "    def restore(self, save_path=None):\n",
    "        \"\"\"ì €ì¥ëœ ëª¨ë¸ ë³µì›\"\"\"\n",
    "        if save_path == None:\n",
    "            save_path = self.save_path\n",
    "        ckpt = tf.train.get_checkpoint_state(save_path)  \n",
    "        if ckpt and ckpt.model_checkpoint_path:  \n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path) \n",
    "            if self.verbose > 0:\n",
    "                print(\"Restored from %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ee7f5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chair Recommendation AutoInt Model Training ===\n",
      "ğŸ¯ Top-k Ranking ìµœì í™” ëª¨ë¸ í›ˆë ¨\n",
      "ì´ ë…¸íŠ¸ë¶ì€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤:\n",
      "1. ë°ì´í„° ì „ì²˜ë¦¬\n",
      "2. K-fold ë¶„í• \n",
      "3. ìŠ¤ì¼€ì¼ë§\n",
      "4. AutoInt + DRM ëª¨ë¸ í›ˆë ¨ (Top-k ìµœì í™”)\n",
      "5. Top-k ì§€í‘œ í‰ê°€ (NDCG@k, Precision@k, Recall@k)\n",
      "6. ëª¨ë¸ ì €ì¥\n",
      "\n",
      "ğŸ“Š í˜„ì¬ ì„¤ì •:\n",
      "   Loss Function: ranking_ndcg\n",
      "   Ranking K: 10\n",
      "   Temperature: 5.0\n",
      "   Batch Size: 512 (ranking loss ìµœì í™”)\n",
      "\n",
      "=== Step 1: ë°ì´í„° ì „ì²˜ë¦¬ ===\n",
      "Processed 100 combinations...\n",
      "Processed 200 combinations...\n",
      "Processed 300 combinations...\n",
      "Processed 400 combinations...\n",
      "Processed 500 combinations...\n",
      "Processed 600 combinations...\n",
      "Processed 700 combinations...\n",
      "Processed 800 combinations...\n",
      "Processed 900 combinations...\n",
      "Processed 1000 combinations...\n",
      "Processed 1100 combinations...\n",
      "Processed 1200 combinations...\n",
      "Processed 1300 combinations...\n",
      "Processed 1400 combinations...\n",
      "Processed 1500 combinations...\n",
      "Processed 1600 combinations...\n",
      "Processed 1700 combinations...\n",
      "Processed 1800 combinations...\n",
      "Processed 1900 combinations...\n",
      "Processed 2000 combinations...\n",
      "Processed 2100 combinations...\n",
      "Processed 2200 combinations...\n",
      "Processed 2300 combinations...\n",
      "Processed 2400 combinations...\n",
      "Processed 2500 combinations...\n",
      "Processed 2600 combinations...\n",
      "Processed 2700 combinations...\n",
      "Processed 2800 combinations...\n",
      "Processed 2900 combinations...\n",
      "Processed 3000 combinations...\n",
      "Processed 3100 combinations...\n",
      "Processed 3200 combinations...\n",
      "Processed 3300 combinations...\n",
      "Processed 3400 combinations...\n",
      "Processed 3500 combinations...\n",
      "Processed 3600 combinations...\n",
      "Processed 3700 combinations...\n",
      "Processed 3800 combinations...\n",
      "Processed 3900 combinations...\n",
      "Processed 4000 combinations...\n",
      "Processed 4100 combinations...\n",
      "Processed 4200 combinations...\n",
      "Processed 4300 combinations...\n",
      "Processed 4400 combinations...\n",
      "Processed 4500 combinations...\n",
      "Processed 4600 combinations...\n",
      "Processed 4700 combinations...\n",
      "Processed 4800 combinations...\n",
      "Processed 4900 combinations...\n",
      "Processed 5000 combinations...\n",
      "Processed 5100 combinations...\n",
      "Processed 5200 combinations...\n",
      "Processed 5300 combinations...\n",
      "Processed 5400 combinations...\n",
      "Processed 5500 combinations...\n",
      "Processed 5600 combinations...\n",
      "Processed 5700 combinations...\n",
      "Processed 5800 combinations...\n",
      "Processed 5900 combinations...\n",
      "Processed 6000 combinations...\n",
      "Processed 6100 combinations...\n",
      "Processed 6200 combinations...\n",
      "Processed 6300 combinations...\n",
      "Processed 6400 combinations...\n",
      "Processed 6500 combinations...\n",
      "Processed 6600 combinations...\n",
      "Processed 6700 combinations...\n",
      "Processed 6800 combinations...\n",
      "Processed 6900 combinations...\n",
      "Processed 7000 combinations...\n",
      "Processed 7100 combinations...\n",
      "Processed 7200 combinations...\n",
      "Processed 7300 combinations...\n",
      "Processed 7400 combinations...\n",
      "Processed 7500 combinations...\n",
      "Processed 7600 combinations...\n",
      "Processed 7700 combinations...\n",
      "Processed 7800 combinations...\n",
      "Processed 7900 combinations...\n",
      "Processed 8000 combinations...\n",
      "Processed 8100 combinations...\n",
      "Processed 8200 combinations...\n",
      "Processed 8300 combinations...\n",
      "Processed 8400 combinations...\n",
      "Processed 8500 combinations...\n",
      "Processed 8600 combinations...\n",
      "Processed 8700 combinations...\n",
      "Processed 8800 combinations...\n",
      "Processed 8900 combinations...\n",
      "Processed 9000 combinations...\n",
      "Processed 9100 combinations...\n",
      "Processed 9200 combinations...\n",
      "Processed 9300 combinations...\n",
      "Processed 9400 combinations...\n",
      "Processed 9500 combinations...\n",
      "Processed 9600 combinations...\n",
      "Processed 9700 combinations...\n",
      "Processed 9800 combinations...\n",
      "Processed 9900 combinations...\n",
      "Processed 10000 combinations...\n",
      "Processed 10100 combinations...\n",
      "Processed 10200 combinations...\n",
      "Processed 10300 combinations...\n",
      "Processed 10400 combinations...\n",
      "Processed 10500 combinations...\n",
      "Processed 10600 combinations...\n",
      "Processed 10700 combinations...\n",
      "Processed 10800 combinations...\n",
      "Processed 10900 combinations...\n",
      "Processed 11000 combinations...\n",
      "Processed 11100 combinations...\n",
      "Processed 11200 combinations...\n",
      "Processed 11300 combinations...\n",
      "Processed 11400 combinations...\n",
      "Processed 11500 combinations...\n",
      "Processed 11600 combinations...\n",
      "Processed 11700 combinations...\n",
      "Processed 11800 combinations...\n",
      "Processed 11900 combinations...\n",
      "Processed 12000 combinations...\n",
      "Processed 12100 combinations...\n",
      "Processed 12200 combinations...\n",
      "Processed 12300 combinations...\n",
      "Processed 12400 combinations...\n",
      "Processed 12500 combinations...\n",
      "Processed 12600 combinations...\n",
      "Processed 12700 combinations...\n",
      "Processed 12800 combinations...\n",
      "Processed 12900 combinations...\n",
      "Processed 13000 combinations...\n",
      "Processed 13100 combinations...\n",
      "Processed 13200 combinations...\n",
      "Processed 13300 combinations...\n",
      "Processed 13400 combinations...\n",
      "Processed 13500 combinations...\n",
      "Processed 13600 combinations...\n",
      "Processed 13700 combinations...\n",
      "Processed 13800 combinations...\n",
      "Processed 13900 combinations...\n",
      "\n",
      "Total combinations: 13986\n",
      "Positive matches: 485 (3.47%)\n",
      "ì „ì²˜ë¦¬ ì™„ë£Œ!\n",
      "\n",
      "=== Step 2: K-fold ë¶„í•  ===\n",
      "Data loading done!\n",
      "Training data: 13986\n",
      "Now part 1\n",
      "Now part 2\n",
      "Now part 3\n",
      "Now part 4\n",
      "Now part 5\n",
      "Now part 6\n",
      "Now part 7\n",
      "Now part 8\n",
      "Now part 9\n",
      "Feature size = 31\n",
      "Now part 1\n",
      "Now part 2\n",
      "Now part 3\n",
      "Now part 4\n",
      "Now part 5\n",
      "Now part 6\n",
      "Now part 7\n",
      "Now part 8\n",
      "Now part 9\n",
      "K-fold ë¶„í•  ì™„ë£Œ!\n",
      "\n",
      "=== Step 3: ìŠ¤ì¼€ì¼ë§ ===\n",
      "Now part 1\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 2\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 3\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 4\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 5\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 6\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 7\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 8\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 9\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ!\n",
      "\n",
      "=== Step 4: AutoInt + DRM ëª¨ë¸ í›ˆë ¨ ===\n",
      "ğŸ¯ Ranking Loss: ranking_ndcg\n",
      "ğŸ“Š Top-10 ìµœì í™”\n",
      "\n",
      "Feature size: 31.0\n",
      "Field size: 24\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/ku/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749763212.792404 19257416 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 3121\n",
      "Extra parameters: 2625\n",
      "\n",
      "=== Epoch 1/5 ===\n",
      "Training on part 3...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.6094\n",
      "  ndcg@10: 0.6194\n",
      "  ndcg@20: 0.6071\n",
      "  precision@5: 0.8000\n",
      "  precision@10: 0.9000\n",
      "  precision@20: 0.8500\n",
      "  recall@5: 0.0038\n",
      "  recall@10: 0.0085\n",
      "  recall@20: 0.0160\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.4584\n",
      "  ndcg@10: 0.5081\n",
      "  ndcg@20: 0.5001\n",
      "  precision@5: 0.2000\n",
      "  precision@10: 0.6000\n",
      "  precision@20: 0.5500\n",
      "  recall@5: 0.0009\n",
      "  recall@10: 0.0057\n",
      "  recall@20: 0.0104\n",
      "[1-1] Model saved! Valid loss improved from 1.0000 to 0.8070\n",
      "[1-1] train-result=0.6194, train-loss=0.8008, valid-result=0.5081, valid-loss=0.8070 [0.4 s]\n",
      "Training on part 4...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.4970\n",
      "  ndcg@10: 0.4801\n",
      "  ndcg@20: 0.4655\n",
      "  precision@5: 0.4000\n",
      "  precision@10: 0.4000\n",
      "  precision@20: 0.3500\n",
      "  recall@5: 0.0019\n",
      "  recall@10: 0.0038\n",
      "  recall@20: 0.0066\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.5911\n",
      "  ndcg@10: 0.5519\n",
      "  ndcg@20: 0.5357\n",
      "  precision@5: 0.6000\n",
      "  precision@10: 0.5000\n",
      "  precision@20: 0.6000\n",
      "  recall@5: 0.0028\n",
      "  recall@10: 0.0047\n",
      "  recall@20: 0.0114\n",
      "[1-2] Model saved! Valid loss improved from 0.8070 to 0.7636\n",
      "[1-2] train-result=0.4801, train-loss=0.7568, valid-result=0.5519, valid-loss=0.7636 [0.1 s]\n",
      "Training on part 5...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.4797\n",
      "  ndcg@10: 0.4813\n",
      "  ndcg@20: 0.5014\n",
      "  precision@5: 0.6000\n",
      "  precision@10: 0.5000\n",
      "  precision@20: 0.5000\n",
      "  recall@5: 0.0028\n",
      "  recall@10: 0.0047\n",
      "  recall@20: 0.0093\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.6047\n",
      "  ndcg@10: 0.5757\n",
      "  ndcg@20: 0.5123\n",
      "  precision@5: 0.6000\n",
      "  precision@10: 0.6000\n",
      "  precision@20: 0.4500\n",
      "  recall@5: 0.0028\n",
      "  recall@10: 0.0057\n",
      "  recall@20: 0.0085\n",
      "[1-3] Model saved! Valid loss improved from 0.7636 to 0.7016\n",
      "[1-3] train-result=0.4813, train-loss=0.7007, valid-result=0.5757, valid-loss=0.7016 [0.2 s]\n",
      "\n",
      "=== Epoch 2/5 ===\n",
      "Training on part 3...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.7017\n",
      "  ndcg@10: 0.6807\n",
      "  ndcg@20: 0.6521\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 0.9000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0094\n",
      "  recall@20: 0.0169\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.6631\n",
      "  ndcg@10: 0.6126\n",
      "  ndcg@20: 0.5877\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 0.8000\n",
      "  precision@20: 0.7000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0076\n",
      "  recall@20: 0.0133\n",
      "[2-1] train-result=0.6807, train-loss=0.7019, valid-result=0.6126, valid-loss=0.7040 [0.1 s]\n",
      "Training on part 4...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.5844\n",
      "  ndcg@10: 0.6287\n",
      "  ndcg@20: 0.6199\n",
      "  precision@5: 0.6000\n",
      "  precision@10: 0.7000\n",
      "  precision@20: 0.7000\n",
      "  recall@5: 0.0028\n",
      "  recall@10: 0.0066\n",
      "  recall@20: 0.0132\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.5691\n",
      "  ndcg@10: 0.5652\n",
      "  ndcg@20: 0.6060\n",
      "  precision@5: 0.6000\n",
      "  precision@10: 0.7000\n",
      "  precision@20: 0.7500\n",
      "  recall@5: 0.0028\n",
      "  recall@10: 0.0066\n",
      "  recall@20: 0.0142\n",
      "[2-2] train-result=0.6287, train-loss=1.0199, valid-result=0.5652, valid-loss=1.0122 [0.1 s]\n",
      "Training on part 5...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 1.0000\n",
      "  ndcg@10: 0.8691\n",
      "  ndcg@20: 0.8387\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 0.8000\n",
      "  precision@20: 0.9000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0075\n",
      "  recall@20: 0.0168\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.8941\n",
      "  ndcg@10: 0.8735\n",
      "  ndcg@20: 0.8477\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 0.9500\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0180\n",
      "[2-3] train-result=0.8691, train-loss=0.7539, valid-result=0.8735, valid-loss=0.7517 [0.1 s]\n",
      "\n",
      "=== Epoch 3/5 ===\n",
      "Training on part 3...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.8493\n",
      "  ndcg@10: 0.7453\n",
      "  ndcg@20: 0.7225\n",
      "  precision@5: 0.8000\n",
      "  precision@10: 0.7000\n",
      "  precision@20: 0.8000\n",
      "  recall@5: 0.0038\n",
      "  recall@10: 0.0066\n",
      "  recall@20: 0.0151\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.8767\n",
      "  ndcg@10: 0.8375\n",
      "  ndcg@20: 0.7837\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 0.9000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0171\n",
      "[3-1] Model saved! Valid loss improved from 0.7016 to 0.6836\n",
      "[3-1] train-result=0.7453, train-loss=0.6842, valid-result=0.8375, valid-loss=0.6836 [0.1 s]\n",
      "Training on part 4...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.6941\n",
      "  ndcg@10: 0.7295\n",
      "  ndcg@20: 0.6984\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 0.9500\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0180\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.6941\n",
      "  ndcg@10: 0.7033\n",
      "  ndcg@20: 0.6968\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 0.9500\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0180\n",
      "[3-2] train-result=0.7295, train-loss=0.6940, valid-result=0.7033, valid-loss=0.6977 [0.1 s]\n",
      "Training on part 5...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.3369\n",
      "  ndcg@10: 0.3713\n",
      "  ndcg@20: 0.4553\n",
      "  precision@5: 0.2000\n",
      "  precision@10: 0.3000\n",
      "  precision@20: 0.4000\n",
      "  recall@5: 0.0009\n",
      "  recall@10: 0.0028\n",
      "  recall@20: 0.0075\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.3982\n",
      "  ndcg@10: 0.4113\n",
      "  ndcg@20: 0.4082\n",
      "  precision@5: 0.4000\n",
      "  precision@10: 0.4000\n",
      "  precision@20: 0.2500\n",
      "  recall@5: 0.0019\n",
      "  recall@10: 0.0038\n",
      "  recall@20: 0.0047\n",
      "[3-3] train-result=0.3713, train-loss=0.6951, valid-result=0.4113, valid-loss=0.6972 [0.1 s]\n",
      "\n",
      "=== Epoch 4/5 ===\n",
      "Training on part 3...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.4322\n",
      "  ndcg@10: 0.4727\n",
      "  ndcg@20: 0.4890\n",
      "  precision@5: 0.2000\n",
      "  precision@10: 0.3000\n",
      "  precision@20: 0.3500\n",
      "  recall@5: 0.0009\n",
      "  recall@10: 0.0028\n",
      "  recall@20: 0.0066\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.7572\n",
      "  ndcg@10: 0.6336\n",
      "  ndcg@20: 0.5932\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 0.6000\n",
      "  precision@20: 0.5000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0057\n",
      "  recall@20: 0.0095\n",
      "[4-1] train-result=0.4727, train-loss=0.6983, valid-result=0.6336, valid-loss=0.6978 [0.1 s]\n",
      "Training on part 4...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.7738\n",
      "  ndcg@10: 0.8089\n",
      "  ndcg@20: 0.8187\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 1.0000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0189\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.8941\n",
      "  ndcg@10: 0.8743\n",
      "  ndcg@20: 0.8620\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 1.0000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0190\n",
      "[4-2] train-result=0.8089, train-loss=0.6957, valid-result=0.8743, valid-loss=0.6951 [0.1 s]\n",
      "Training on part 5...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.8203\n",
      "  ndcg@10: 0.8400\n",
      "  ndcg@20: 0.8266\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 1.0000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0093\n",
      "  recall@20: 0.0187\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.9059\n",
      "  ndcg@10: 0.9105\n",
      "  ndcg@20: 0.8938\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 1.0000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0190\n",
      "[4-3] Model saved! Valid loss improved from 0.6836 to 0.6821\n",
      "[4-3] train-result=0.8400, train-loss=0.6804, valid-result=0.9105, valid-loss=0.6821 [0.1 s]\n",
      "\n",
      "=== Epoch 5/5 ===\n",
      "Training on part 3...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.8602\n",
      "  ndcg@10: 0.8545\n",
      "  ndcg@20: 0.8634\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 1.0000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0094\n",
      "  recall@20: 0.0188\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.8555\n",
      "  ndcg@10: 0.8802\n",
      "  ndcg@20: 0.8884\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 1.0000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0190\n",
      "[5-1] train-result=0.8545, train-loss=0.7009, valid-result=0.8802, valid-loss=0.7007 [0.1 s]\n",
      "Training on part 4...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.3923\n",
      "  ndcg@10: 0.4396\n",
      "  ndcg@20: 0.4705\n",
      "  precision@5: 0.2000\n",
      "  precision@10: 0.3000\n",
      "  precision@20: 0.5000\n",
      "  recall@5: 0.0009\n",
      "  recall@10: 0.0028\n",
      "  recall@20: 0.0095\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.4882\n",
      "  ndcg@10: 0.4449\n",
      "  ndcg@20: 0.4564\n",
      "  precision@5: 0.6000\n",
      "  precision@10: 0.4000\n",
      "  precision@20: 0.4000\n",
      "  recall@5: 0.0028\n",
      "  recall@10: 0.0038\n",
      "  recall@20: 0.0076\n",
      "[5-2] train-result=0.4396, train-loss=0.8260, valid-result=0.4449, valid-loss=0.8212 [0.1 s]\n",
      "Training on part 5...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.5144\n",
      "  ndcg@10: 0.5856\n",
      "  ndcg@20: 0.6048\n",
      "  precision@5: 0.8000\n",
      "  precision@10: 0.9000\n",
      "  precision@20: 0.9500\n",
      "  recall@5: 0.0037\n",
      "  recall@10: 0.0084\n",
      "  recall@20: 0.0177\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.5410\n",
      "  ndcg@10: 0.5322\n",
      "  ndcg@20: 0.5713\n",
      "  precision@5: 0.6000\n",
      "  precision@10: 0.6000\n",
      "  precision@20: 0.7500\n",
      "  recall@5: 0.0028\n",
      "  recall@10: 0.0057\n",
      "  recall@20: 0.0142\n",
      "[5-3] train-result=0.5856, train-loss=0.9113, valid-result=0.5322, valid-loss=0.9061 [0.1 s]\n",
      "\n",
      "=== ğŸ‰ í›ˆë ¨ ì™„ë£Œ! ===\n",
      "ğŸ¯ Top-k ìµœì í™” ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: ./saved_models/autoint/2/\n",
      "ğŸ“Š ì‚¬ìš©ëœ ì§€í‘œ: NDCG@10, Precision@10, Recall@10\n",
      "\n",
      "âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\n",
      "\n",
      "=== ğŸ ì™„ë£Œ ===\n",
      "ğŸ¯ Top-k ìµœì í™” AutoInt ëª¨ë¸ í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "\n",
      "ğŸ’¡ ì´ì œ ì›¹ ì„œë¹„ìŠ¤ì—ì„œ ë‹¤ìŒ ì„¤ì •ì„ ì‚¬ìš©í•˜ì„¸ìš”:\n",
      "   model_path = './saved_models/autoint/2/'\n",
      "   ranking_optimized = True\n",
      "   top_k_performance = 'NDCG@10 optimized'\n",
      "\n",
      "ğŸš€ ì¶”ì²œ ì‹œìŠ¤í…œ ì„±ëŠ¥:\n",
      "   âœ… ìƒìœ„ ë­í‚¹ ì •í™•ë„ ê·¹ëŒ€í™”\n",
      "   âœ… ê°œì¸í™” ì¶”ì²œ í’ˆì§ˆ í–¥ìƒ\n",
      "   âœ… Top-k ì§€í‘œ ìµœì í™” ì™„ë£Œ\n",
      "   âœ… ì‹¤ì‹œê°„ ì›¹ ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7. ì‹¤í–‰ ì½”ë“œ\n",
    "# ============================================================================\n",
    "\n",
    "# ì„¤ì • ì •ì˜ - Ranking Loss ì‚¬ìš©\n",
    "args = Namespace(\n",
    "    is_save=True,\n",
    "    greater_is_better=True,\n",
    "    has_residual=True,\n",
    "    blocks=2,\n",
    "    block_shape=[16, 16],\n",
    "    heads=2,\n",
    "    embedding_size=16,\n",
    "    dropout_keep_prob=[0.8, 0.8, 0.5],  # ë“œë¡­ì•„ì›ƒ ì•½ê°„ ì ìš©\n",
    "    epoch=5,  # ì¡°ê¸ˆ ë” ë§ì€ ì—í­\n",
    "    batch_size=512,  # ë°°ì¹˜ í¬ê¸° ì¤„ì„ (ranking lossê°€ ë©”ëª¨ë¦¬ë¥¼ ë” ì‚¬ìš©)\n",
    "    learning_rate=0.001,\n",
    "    optimizer_type='adam',\n",
    "    l2_reg=0.001,\n",
    "    random_seed=42,\n",
    "    save_path=MODEL_SAVE_PATH,\n",
    "    field_size=24,\n",
    "    \n",
    "    # â­ Top-k Ranking Loss ì„¤ì •\n",
    "    loss_type='ranking_ndcg',      # 'ranking_ndcg' ë˜ëŠ” 'ranking_precision'\n",
    "    ranking_k=10,                  # Top-10ìœ¼ë¡œ í‰ê°€\n",
    "    ranking_tau=5.0,               # Neural sort temperature (ë‚®ì„ìˆ˜ë¡ sharp)\n",
    "    \n",
    "    verbose=1,\n",
    "    run_times=1,\n",
    "    deep_layers=None,              # ë˜ëŠ” [128, 64] ë“± DNN ì¶”ê°€ ê°€ëŠ¥\n",
    "    batch_norm=0,\n",
    "    batch_norm_decay=0.995\n",
    ")\n",
    "\n",
    "print(\"=== Chair Recommendation AutoInt Model Training ===\")\n",
    "print(\"ğŸ¯ Top-k Ranking ìµœì í™” ëª¨ë¸ í›ˆë ¨\")\n",
    "print(\"ì´ ë…¸íŠ¸ë¶ì€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤:\")\n",
    "print(\"1. ë°ì´í„° ì „ì²˜ë¦¬\")\n",
    "print(\"2. K-fold ë¶„í• \") \n",
    "print(\"3. ìŠ¤ì¼€ì¼ë§\")\n",
    "print(\"4. AutoInt + DRM ëª¨ë¸ í›ˆë ¨ (Top-k ìµœì í™”)\")\n",
    "print(\"5. Top-k ì§€í‘œ í‰ê°€ (NDCG@k, Precision@k, Recall@k)\")\n",
    "print(\"6. ëª¨ë¸ ì €ì¥\")\n",
    "print()\n",
    "print(f\"ğŸ“Š í˜„ì¬ ì„¤ì •:\")\n",
    "print(f\"   Loss Function: {args.loss_type}\")\n",
    "print(f\"   Ranking K: {args.ranking_k}\")\n",
    "print(f\"   Temperature: {args.ranking_tau}\")\n",
    "print(f\"   Batch Size: {args.batch_size} (ranking loss ìµœì í™”)\")\n",
    "print()\n",
    "\n",
    "# ë‹¨ê³„ë³„ ì‹¤í–‰\n",
    "run_preprocessing = True  # ì „ì²˜ë¦¬ ì‹¤í–‰ ì—¬ë¶€\n",
    "run_training = True       # í›ˆë ¨ ì‹¤í–‰ ì—¬ë¶€\n",
    "\n",
    "if run_preprocessing:\n",
    "    print(\"=== Step 1: ë°ì´í„° ì „ì²˜ë¦¬ ===\")\n",
    "    # ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”\n",
    "    preprocessor = DataPreprocessor(\n",
    "        chair_path='ohouse_chair.xlsx',  # ì‹¤ì œ ì˜ì ë°ì´í„° íŒŒì¼\n",
    "        person_path='person.csv',       # ì‹¤ì œ ì‚¬ëŒ ë°ì´í„° íŒŒì¼\n",
    "        output_dir=DATA_PATH\n",
    "    )\n",
    "    preprocessor.process_data()\n",
    "    print(\"ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "    print()\n",
    "\n",
    "    print(\"=== Step 2: K-fold ë¶„í•  ===\")\n",
    "    train_x, train_y = _load_data()\n",
    "    folds = list(KFold(n_splits=9, shuffle=True, random_state=args.random_seed).split(train_x))\n",
    "    fold_index = [valid_id for train_id, valid_id in folds]\n",
    "    \n",
    "    np.save(DATA_PATH + \"fold_index.npy\", np.array(fold_index))\n",
    "    save_x_y(fold_index, train_x, train_y)\n",
    "    save_i(fold_index)\n",
    "    print(\"K-fold ë¶„í•  ì™„ë£Œ!\")\n",
    "    print()\n",
    "\n",
    "    print(\"=== Step 3: ìŠ¤ì¼€ì¼ë§ ===\")\n",
    "    scale_each_fold()\n",
    "    print(\"ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ!\")\n",
    "    print()\n",
    "\n",
    "if run_training:\n",
    "    print(\"=== Step 4: AutoInt + DRM ëª¨ë¸ í›ˆë ¨ ===\")\n",
    "    print(f\"ğŸ¯ Ranking Loss: {args.loss_type}\")\n",
    "    print(f\"ğŸ“Š Top-{args.ranking_k} ìµœì í™”\")\n",
    "    print()\n",
    "    \n",
    "    # íŠ¹ì„± í¬ê¸° ë¡œë“œ\n",
    "    feature_size = np.load(DATA_PATH + '/feature_size.npy')[0]\n",
    "    print(f\"Feature size: {feature_size}\")\n",
    "    \n",
    "    # ê²€ì¦ ë°ì´í„° ë¡œë“œ\n",
    "    Xi_valid = np.load(DATA_PATH + '/part2/train_i.npy')\n",
    "    Xv_valid = np.load(DATA_PATH + '/part2/train_x2.npy')\n",
    "    y_valid = np.load(DATA_PATH + '/part2/train_y.npy')\n",
    "    \n",
    "    # ì‹¤ì œ field_size ì„¤ì •\n",
    "    args.field_size = Xi_valid.shape[1]\n",
    "    print(f\"Field size: {args.field_size}\")\n",
    "    \n",
    "    # ëª¨ë¸ ìƒì„±\n",
    "    model = AutoInt(args=args, feature_size=feature_size, run_cnt=1)\n",
    "    \n",
    "    # í›ˆë ¨ ì‹œì‘\n",
    "    for k in range(args.epoch):\n",
    "        print(f\"\\n=== Epoch {k+1}/{args.epoch} ===\")\n",
    "        \n",
    "        for j in range(3, 6):  # part3-5ë§Œ ì‚¬ìš© (ë¹ ë¥¸ í›ˆë ¨ì„ ìœ„í•´)\n",
    "            print(f\"Training on part {j}...\")\n",
    "            \n",
    "            Xi_train = np.load(DATA_PATH + f'/part{j}/train_i.npy')\n",
    "            Xv_train = np.load(DATA_PATH + f'/part{j}/train_x2.npy')\n",
    "            y_train = np.load(DATA_PATH + f'/part{j}/train_y.npy')\n",
    "            \n",
    "            model.fit_once(Xi_train, Xv_train, y_train, k+1, j-2,\n",
    "                          Xi_valid, Xv_valid, y_valid, early_stopping=True)\n",
    "    \n",
    "    print(\"\\n=== ğŸ‰ í›ˆë ¨ ì™„ë£Œ! ===\")\n",
    "    print(f\"ğŸ¯ Top-k ìµœì í™” ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {MODEL_SAVE_PATH}2/\")\n",
    "    print(f\"ğŸ“Š ì‚¬ìš©ëœ ì§€í‘œ: NDCG@{args.ranking_k}, Precision@{args.ranking_k}, Recall@{args.ranking_k}\")\n",
    "    print()\n",
    "    \n",
    "    # ìµœì¢… ê²€ì¦\n",
    "    if os.path.exists(MODEL_SAVE_PATH + '2/checkpoint'):\n",
    "        print(\"âœ… ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸ ì™„ë£Œ\")\n",
    "        print(\"ğŸš€ ì›¹ ì„œë¹„ìŠ¤ì—ì„œ Top-k ì¶”ì²œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤!\")\n",
    "        print()\n",
    "        print(\"ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼:\")\n",
    "        print(\"   - ìƒìœ„ ë­í‚¹ ì •í™•ë„ í–¥ìƒ\")\n",
    "        print(\"   - ì‚¬ìš©ì ë§ì¶¤ ì¶”ì²œ í’ˆì§ˆ ê°œì„ \") \n",
    "        print(\"   - NDCG, Precision, Recall ì§€í‘œ ìµœì í™”\")\n",
    "    else:\n",
    "        print(\"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n",
    "\n",
    "print(\"\\n=== ğŸ ì™„ë£Œ ===\")\n",
    "print(\"ğŸ¯ Top-k ìµœì í™” AutoInt ëª¨ë¸ í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print()\n",
    "print(\"ğŸ’¡ ì´ì œ ì›¹ ì„œë¹„ìŠ¤ì—ì„œ ë‹¤ìŒ ì„¤ì •ì„ ì‚¬ìš©í•˜ì„¸ìš”:\")\n",
    "print(f\"   model_path = '{MODEL_SAVE_PATH}2/'\")\n",
    "print(f\"   ranking_optimized = True\")\n",
    "print(f\"   top_k_performance = 'NDCG@{args.ranking_k} optimized'\")\n",
    "print()\n",
    "print(\"ğŸš€ ì¶”ì²œ ì‹œìŠ¤í…œ ì„±ëŠ¥:\")\n",
    "print(\"   âœ… ìƒìœ„ ë­í‚¹ ì •í™•ë„ ê·¹ëŒ€í™”\")\n",
    "print(\"   âœ… ê°œì¸í™” ì¶”ì²œ í’ˆì§ˆ í–¥ìƒ\")\n",
    "print(\"   âœ… Top-k ì§€í‘œ ìµœì í™” ì™„ë£Œ\")\n",
    "print(\"   âœ… ì‹¤ì‹œê°„ ì›¹ ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9040b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ku",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
