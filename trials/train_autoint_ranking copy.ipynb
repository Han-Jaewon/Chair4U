{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609e5886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/ku/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. 라이브러리 및 설정\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from time import time\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score, log_loss, ndcg_score\n",
    "from sklearn.model_selection import KFold\n",
    "from argparse import Namespace\n",
    "\n",
    "# 경로 설정\n",
    "DATA_PATH = './processed_data/'\n",
    "MODEL_SAVE_PATH = './saved_models/autoint/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2e2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. 전처리 함수들\n",
    "# ============================================================================\n",
    "\n",
    "def safe_divide(numerator, denominator, default=0.0):\n",
    "    \"\"\"안전한 나눗셈 헬퍼 함수\"\"\"\n",
    "    if denominator > 0:\n",
    "        return numerator / denominator\n",
    "    return default\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, chair_path, person_path, output_dir):\n",
    "        self.chair_path = chair_path\n",
    "        self.person_path = person_path\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # 사용할 Chair 피처 정의\n",
    "        self.categorical_features = [\n",
    "            '헤드레스트 유무', '팔걸이 유무', '요추지지대 유무', \n",
    "            '높이 조절 레버 유무', '틸팅 여부', '등받이 곧/꺾'\n",
    "        ]\n",
    "        \n",
    "        self.numerical_features = [\n",
    "            'h8_지면-좌석 높이_MIN', 'h8_지면-좌석 높이_MAX',\n",
    "            'b3_좌석 가로 길이', 't4_좌석 세로 길이 일반',\n",
    "            'b4_등받이 가로 길이', 'h7_등받이 세로 길이'\n",
    "        ]\n",
    "        \n",
    "        self.person_features = [\n",
    "            'human-height', 'A_Buttock-popliteal length',\n",
    "            'B_Popliteal-height', 'C_Hip-breadth',\n",
    "            'F_Sitting-height', 'G_Shoulder-breadth'\n",
    "        ]\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"데이터 로드 및 기본 전처리\"\"\"\n",
    "        # Chair 데이터 로드\n",
    "        self.chair_df = pd.read_excel(self.chair_path, engine='openpyxl')\n",
    "        \n",
    "        # Person 데이터 로드 (cm -> mm 변환)\n",
    "        self.person_df = pd.read_csv(self.person_path, encoding='utf-8')\n",
    "        for col in self.person_features:\n",
    "            if col in self.person_df.columns:\n",
    "                self.person_df[col] *= 10  # cm to mm\n",
    "        \n",
    "        # 범주형 피처 전처리\n",
    "        for col in self.categorical_features:\n",
    "            if col in self.chair_df.columns:\n",
    "                if col == '등받이 곧/꺾':\n",
    "                    self.chair_df[col] = self.chair_df[col].map({'곧': 0, '꺾': 1})\n",
    "                else:\n",
    "                    self.chair_df[col] = self.chair_df[col].map({'O': 1, 'X': 0})\n",
    "        \n",
    "        # 결측값 처리\n",
    "        self.chair_df['h8_지면-좌석 높이_MAX'] = np.where(pd.isna(self.chair_df['h8_지면-좌석 높이_MAX']),\n",
    "                                                        self.chair_df['h8_지면-좌석 높이_MIN'],\n",
    "                                                        self.chair_df['h8_지면-좌석 높이_MAX'])\n",
    "        self.chair_df[self.categorical_features] = self.chair_df[self.categorical_features].fillna(0)\n",
    "        self.chair_df[self.numerical_features] = self.chair_df[self.numerical_features].fillna(self.chair_df[self.numerical_features].mean())\n",
    "\n",
    "    def create_feature_mappings(self):\n",
    "        \"\"\"피처 인덱스 매핑 생성\"\"\"\n",
    "        self.feature_idx_map = {}\n",
    "        idx = 1\n",
    "        \n",
    "        # Person 피처 (6개)\n",
    "        for feat in self.person_features:\n",
    "            self.feature_idx_map[f'person_{feat}'] = idx\n",
    "            idx += 1\n",
    "        \n",
    "        # Chair 수치형 피처 (6개)\n",
    "        for feat in self.numerical_features:\n",
    "            self.feature_idx_map[f'chair_{feat}'] = idx\n",
    "            idx += 1\n",
    "        \n",
    "        # 상호작용 피처 (6개)\n",
    "        interaction_features = [\n",
    "            'height_match_score', 'width_margin_ratio', 'depth_margin_ratio',\n",
    "            'backrest_height_ratio', 'shoulder_width_ratio', 'adjustable_range'\n",
    "        ]\n",
    "        for feat in interaction_features:\n",
    "            self.feature_idx_map[feat] = idx\n",
    "            idx += 1\n",
    "        \n",
    "        # 이진 범주형 피처 시작 오프셋\n",
    "        self.binary_offset = idx\n",
    "        \n",
    "        # 이진 피처 인덱스\n",
    "        for i, feat in enumerate(self.categorical_features):\n",
    "            self.feature_idx_map[f'{feat}_0'] = self.binary_offset + i * 2\n",
    "            self.feature_idx_map[f'{feat}_1'] = self.binary_offset + i * 2 + 1\n",
    "\n",
    "    def calculate_interaction_features(self, person_row, chair_row):\n",
    "        \"\"\"Person과 Chair 간의 상호작용 피처 계산\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        h8_mid = (chair_row['h8_지면-좌석 높이_MIN'] + chair_row['h8_지면-좌석 높이_MAX']) / 2\n",
    "        h8_range = chair_row['h8_지면-좌석 높이_MAX'] - chair_row['h8_지면-좌석 높이_MIN']\n",
    "        popliteal_height = person_row['B_Popliteal-height']\n",
    "        \n",
    "        if h8_range > 0:\n",
    "            if chair_row['h8_지면-좌석 높이_MIN'] <= popliteal_height <= chair_row['h8_지면-좌석 높이_MAX']:\n",
    "                features['height_match_score'] = 1.0\n",
    "            else:\n",
    "                if popliteal_height < chair_row['h8_지면-좌석 높이_MIN']:\n",
    "                    dist = chair_row['h8_지면-좌석 높이_MIN'] - popliteal_height\n",
    "                else:\n",
    "                    dist = popliteal_height - chair_row['h8_지면-좌석 높이_MAX']\n",
    "                features['height_match_score'] = max(0, 1 - dist / 100)\n",
    "        else:\n",
    "            features['height_match_score'] = max(0, 1 - abs(h8_mid - popliteal_height) / 50)\n",
    "        \n",
    "        features['width_margin_ratio'] = safe_divide(\n",
    "            chair_row['b3_좌석 가로 길이'] - person_row['C_Hip-breadth'], \n",
    "            person_row['C_Hip-breadth']\n",
    "        )\n",
    "        features['depth_margin_ratio'] = safe_divide(\n",
    "            person_row['A_Buttock-popliteal length'] - chair_row['t4_좌석 세로 길이 일반'], \n",
    "            person_row['A_Buttock-popliteal length']\n",
    "        )\n",
    "        features['backrest_height_ratio'] = safe_divide(\n",
    "            chair_row['h7_등받이 세로 길이'], \n",
    "            person_row['F_Sitting-height']\n",
    "        )\n",
    "        features['shoulder_width_ratio'] = safe_divide(\n",
    "            chair_row['b4_등받이 가로 길이'], \n",
    "            person_row['G_Shoulder-breadth']\n",
    "        )\n",
    "        features['adjustable_range'] = h8_range\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def check_matching_conditions(self, person_row, chair_row):\n",
    "        \"\"\"필수 매칭 조건 확인 및 레이블 생성\"\"\"\n",
    "        conditions = {\n",
    "            't4 < A': chair_row['t4_좌석 세로 길이 일반'] < person_row['A_Buttock-popliteal length'],\n",
    "            'h8 ≈ B': (chair_row['h8_지면-좌석 높이_MIN'] <= person_row['B_Popliteal-height'] <= chair_row['h8_지면-좌석 높이_MAX']) \n",
    "                      if chair_row['h8_지면-좌석 높이_MAX'] > chair_row['h8_지면-좌석 높이_MIN']\n",
    "                      else abs((chair_row['h8_지면-좌석 높이_MIN'] + chair_row['h8_지면-좌석 높이_MAX'])/2 - person_row['B_Popliteal-height']) < 50,\n",
    "            'b3 > C': chair_row['b3_좌석 가로 길이'] > person_row['C_Hip-breadth'],\n",
    "            'h7 < F': chair_row['h7_등받이 세로 길이'] < person_row['F_Sitting-height'],\n",
    "            'b4 ≥ G': chair_row['b4_등받이 가로 길이'] >= person_row['G_Shoulder-breadth']\n",
    "        }\n",
    "        \n",
    "        all_satisfied = all(conditions.values())\n",
    "        soft_label = sum(conditions.values()) / len(conditions)\n",
    "        \n",
    "        return int(all_satisfied), soft_label, conditions\n",
    "    \n",
    "    def process_data(self):\n",
    "        \"\"\"전체 데이터 처리 및 파일 생성\"\"\"\n",
    "        self.load_data()\n",
    "        self.create_feature_mappings()\n",
    "        \n",
    "        f_train_value = open(os.path.join(self.output_dir, 'train_x.txt'), 'w')\n",
    "        f_train_index = open(os.path.join(self.output_dir, 'train_i.txt'), 'w')\n",
    "        f_train_label = open(os.path.join(self.output_dir, 'train_y.txt'), 'w')\n",
    "        \n",
    "        cnt = 0\n",
    "        positive_cnt = 0\n",
    "        \n",
    "        for _, person in self.person_df.iterrows():\n",
    "            for _, chair in self.chair_df.iterrows():\n",
    "                cnt += 1\n",
    "                \n",
    "                values = []\n",
    "                indices = []\n",
    "                \n",
    "                # Person 수치형 피처\n",
    "                for feat in self.person_features:\n",
    "                    if feat in person.index:\n",
    "                        values.append(str(person[feat]))\n",
    "                        indices.append(str(self.feature_idx_map[f'person_{feat}']))\n",
    "                \n",
    "                # Chair 수치형 피처\n",
    "                for feat in self.numerical_features:\n",
    "                    values.append(str(chair[feat]))\n",
    "                    indices.append(str(self.feature_idx_map[f'chair_{feat}']))\n",
    "                \n",
    "                # 상호작용 피처\n",
    "                interaction_feats = self.calculate_interaction_features(person, chair)\n",
    "                for feat_name, feat_value in interaction_feats.items():\n",
    "                    values.append(str(feat_value))\n",
    "                    indices.append(str(self.feature_idx_map[feat_name]))\n",
    "                \n",
    "                # 이진 범주형 피처\n",
    "                for feat in self.categorical_features:\n",
    "                    values.append('1')\n",
    "                    feat_value = int(chair[feat]) if not pd.isna(chair[feat]) else 0\n",
    "                    idx_key = f'{feat}_{feat_value}'\n",
    "                    indices.append(str(self.feature_idx_map[idx_key]))\n",
    "                \n",
    "                # 레이블 계산\n",
    "                hard_label, soft_label, conditions = self.check_matching_conditions(person, chair)\n",
    "                \n",
    "                f_train_value.write(' '.join(values) + '\\n')\n",
    "                f_train_index.write(' '.join(indices) + '\\n')\n",
    "                f_train_label.write(f'{soft_label:.4f}\\n')\n",
    "                \n",
    "                if hard_label == 1:\n",
    "                    positive_cnt += 1\n",
    "                \n",
    "                if cnt % 100 == 0:\n",
    "                    print(f'Processed {cnt} combinations...')\n",
    "        \n",
    "        f_train_value.close()\n",
    "        f_train_index.close()\n",
    "        f_train_label.close()\n",
    "        \n",
    "        print(f\"\\nTotal combinations: {cnt}\")\n",
    "        print(f\"Positive matches: {positive_cnt} ({positive_cnt/cnt*100:.2f}%)\")\n",
    "        \n",
    "        # 메타데이터 저장\n",
    "        metadata = {\n",
    "            'feature_mappings': self.feature_idx_map,\n",
    "            'total_features': len(self.feature_idx_map),\n",
    "            'numerical_features': self.binary_offset - 1,\n",
    "            'categorical_features': self.categorical_features,\n",
    "            'person_features': self.person_features,\n",
    "            'chair_numerical_features': self.numerical_features\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, 'metadata.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4605b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. K-fold 분할\n",
    "# ============================================================================\n",
    "\n",
    "def _load_data(_nrows=None):\n",
    "    \"\"\"train_x.txt와 train_y.txt 파일을 읽어 데이터를 로드\"\"\"\n",
    "    train_x = pd.read_csv(DATA_PATH + 'train_x.txt', header=None, sep=' ', nrows=_nrows, dtype=np.float64)\n",
    "    train_y = pd.read_csv(DATA_PATH + 'train_y.txt', header=None, sep=' ', nrows=_nrows, dtype=np.float64)\n",
    "    \n",
    "    train_x = train_x.values\n",
    "    train_y = train_y.values.reshape([-1])\n",
    "    \n",
    "    print('Data loading done!')\n",
    "    print('Training data: %d' % train_y.shape[0])\n",
    "    \n",
    "    assert train_x.shape[0] == train_y.shape[0]\n",
    "    return train_x, train_y\n",
    "\n",
    "def save_x_y(fold_index, train_x, train_y):\n",
    "    \"\"\"10개의 fold로 분할된 데이터를 각각 저장\"\"\"\n",
    "    _get = lambda x, l: [x[i] for i in l]\n",
    "\n",
    "    for i in range(len(fold_index)):\n",
    "        print(\"Now part %d\" % (i+1))\n",
    "        part_index = fold_index[i]\n",
    "        \n",
    "        Xv_train_, y_train_ = _get(train_x, part_index), _get(train_y, part_index)\n",
    "        \n",
    "        save_dir = DATA_PATH + \"part\" + str(i+1) + \"/\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "        np.save(save_dir + 'train_x.npy', Xv_train_)\n",
    "        np.save(save_dir + 'train_y.npy', y_train_)\n",
    "\n",
    "def save_i(fold_index):\n",
    "    \"\"\"train_i.txt 파일을 읽어 범주형 특성 인덱스를 fold별로 저장\"\"\"\n",
    "    _get = lambda x, l: [x[i] for i in l]\n",
    "    \n",
    "    train_i = pd.read_csv(DATA_PATH + 'train_i.txt', header=None, sep=' ', dtype=np.float64)\n",
    "    train_i = train_i.values\n",
    "    \n",
    "    feature_size = train_i.max() + 1\n",
    "    print(\"Feature size = %d\" % feature_size)\n",
    "    \n",
    "    np.save(DATA_PATH + \"feature_size.npy\", np.array([feature_size]))\n",
    "    \n",
    "    for i in range(len(fold_index)):\n",
    "        print(\"Now part %d\" % (i+1))\n",
    "        part_index = fold_index[i]\n",
    "        Xi_train_ = _get(train_i, part_index)\n",
    "        np.save(DATA_PATH + \"part\" + str(i+1) + '/train_i.npy', Xi_train_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1b3e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. 스케일링\n",
    "# ============================================================================\n",
    "\n",
    "def scale(x):\n",
    "    \"\"\"개별 수치형 피처를 스케일링하는 함수\"\"\"\n",
    "    if x > 2:\n",
    "        x = int(math.log(float(x))**2)\n",
    "    return x\n",
    "\n",
    "def scale_each_fold():\n",
    "    \"\"\"10개의 fold에 대해 각각 스케일링을 수행\"\"\"\n",
    "    for i in range(1, 10): # 수정전: 11\n",
    "        print('Now part %d' % i)\n",
    "        \n",
    "        data = np.load(DATA_PATH + 'part'+str(i)+'/train_x.npy')\n",
    "        part = data[:, 0:18]  # 처음 18개 수치형 피처만\n",
    "        \n",
    "        for j in range(part.shape[0]):\n",
    "            if j % 100 == 0:\n",
    "                print(j)\n",
    "            part[j] = list(map(scale, part[j]))\n",
    "        \n",
    "        np.save(DATA_PATH + 'part' + str(i) + '/train_x2.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8578613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. DRM (Differentiable Ranking Metrics) 구현\n",
    "# ============================================================================\n",
    "\n",
    "class DifferentiableRankingLoss:\n",
    "    \"\"\"TensorFlow implementation of differentiable ranking metrics for top-k\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def detNeuralSort(s, tau=1.0, k=5):\n",
    "        \"\"\"Deterministic neural sort for ranking\"\"\"\n",
    "        batch_size = tf.shape(s)[0]\n",
    "        n = tf.shape(s)[1]\n",
    "        \n",
    "        # Expand dimensions for broadcasting\n",
    "        su = tf.expand_dims(s, axis=-1)  # [batch_size, n_items, 1]\n",
    "        \n",
    "        # Create matrices\n",
    "        one = tf.ones([n, 1], dtype=tf.float32)\n",
    "        one_k = tf.ones([1, k], dtype=tf.float32)\n",
    "        \n",
    "        # Compute A_s = |s_i - s_j|\n",
    "        A_s = tf.abs(su - tf.transpose(su, [0, 2, 1]))  # [batch_size, n, n]\n",
    "        \n",
    "        # Compute B\n",
    "        B = tf.matmul(A_s, tf.matmul(one, one_k))  # [batch_size, n, k]\n",
    "        \n",
    "        # Compute scaling\n",
    "        scaling = tf.cast(n + 1 - 2 * (tf.range(n) + 1), tf.float32)\n",
    "        scaling = tf.expand_dims(scaling, 0)  # [1, n]\n",
    "        \n",
    "        # Compute C\n",
    "        C = tf.expand_dims(s * scaling, -1)[:, :, :k]  # [batch_size, n, k]\n",
    "        \n",
    "        # Compute P_max\n",
    "        P_max = tf.transpose(C - B, [0, 2, 1])  # [batch_size, k, n]\n",
    "        \n",
    "        # Apply softmax\n",
    "        P_hat = tf.nn.softmax(P_max / tau, axis=-1)\n",
    "        \n",
    "        return P_hat\n",
    "    \n",
    "    @staticmethod\n",
    "    def neuNDCGLoss(scores, labels, k=5, tau=10.0):\n",
    "        \"\"\"Neural NDCG Loss for ranking\"\"\"\n",
    "        batch_size = tf.shape(scores)[0]\n",
    "        n_items = tf.shape(scores)[1]\n",
    "        \n",
    "        # Create discount matrix\n",
    "        discounts = 1.0 / tf.math.log(tf.range(2, k + 2, dtype=tf.float32))  # log base is e, so we use range(2, k+2)\n",
    "        diag = tf.linalg.diag(discounts)\n",
    "        \n",
    "        # Get top-k for efficiency (사실상 전체를 다 사용하지만 구조상 필요)\n",
    "        k_actual = tf.minimum(k, n_items)\n",
    "        \n",
    "        # Neural sort 적용\n",
    "        P_hat = DifferentiableRankingLoss.detNeuralSort(scores, tau=tau, k=k_actual)\n",
    "        \n",
    "        # IDCG 계산 (Ideal DCG)\n",
    "        sorted_labels, _ = tf.nn.top_k(labels, k=k_actual)\n",
    "        ideal_dcg = tf.reduce_sum(sorted_labels * discounts[:k_actual], axis=1)\n",
    "        \n",
    "        # DCG 계산\n",
    "        # P_hat: [batch_size, k, n_items], labels: [batch_size, n_items]\n",
    "        # 각 포지션에서의 기대 레이블 값 계산\n",
    "        expected_labels_at_positions = tf.matmul(P_hat, tf.expand_dims(labels, -1))  # [batch_size, k, 1]\n",
    "        expected_labels_at_positions = tf.squeeze(expected_labels_at_positions, -1)  # [batch_size, k]\n",
    "        \n",
    "        # DCG 계산\n",
    "        dcg = tf.reduce_sum(expected_labels_at_positions * discounts[:k_actual], axis=1)\n",
    "        \n",
    "        # NDCG 계산 (0으로 나누기 방지)\n",
    "        ndcg = dcg / (ideal_dcg + 1e-10)\n",
    "        \n",
    "        # Loss는 -NDCG (최대화하기 위해)\n",
    "        loss = -tf.reduce_mean(ndcg)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def neuPrecisionLoss(scores, labels, k=5, tau=10.0):\n",
    "        \"\"\"Neural Precision@k Loss\"\"\"\n",
    "        batch_size = tf.shape(scores)[0]\n",
    "        n_items = tf.shape(scores)[1]\n",
    "        k_actual = tf.minimum(k, n_items)\n",
    "        \n",
    "        # Neural sort\n",
    "        P_hat = DifferentiableRankingLoss.detNeuralSort(scores, tau=tau, k=k_actual)\n",
    "        \n",
    "        # Precision@k 계산\n",
    "        # P_hat: [batch_size, k, n_items]에서 각 포지션의 기대 레이블 값\n",
    "        expected_labels = tf.matmul(P_hat, tf.expand_dims(labels, -1))  # [batch_size, k, 1]\n",
    "        expected_labels = tf.squeeze(expected_labels, -1)  # [batch_size, k]\n",
    "        \n",
    "        # Precision = (top-k에서 relevant한 것들의 수) / k\n",
    "        precision = tf.reduce_sum(expected_labels, axis=1) / tf.cast(k_actual, tf.float32)\n",
    "        \n",
    "        # Loss는 -Precision\n",
    "        loss = -tf.reduce_mean(precision)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c46e734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. AutoInt 모델 정의 (Ranking Loss 포함)\n",
    "# ============================================================================\n",
    "\n",
    "def normalize(inputs, epsilon=1e-8):\n",
    "    \"\"\"Layer Normalization\"\"\"\n",
    "    inputs_shape = inputs.get_shape()\n",
    "    params_shape = inputs_shape[-1:]\n",
    "\n",
    "    mean, variance = tf.nn.moments(inputs, [-1], keepdims=True)\n",
    "    beta = tf.Variable(tf.zeros(params_shape))\n",
    "    gamma = tf.Variable(tf.ones(params_shape))\n",
    "    normalized = (inputs - mean) / ((variance + epsilon) ** (.5))\n",
    "    outputs = gamma * normalized + beta\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def multihead_attention(queries, keys, values, num_units=None, num_heads=1,\n",
    "                        dropout_keep_prob=1, is_training=True, has_residual=True):\n",
    "    \"\"\"Multi-head Self-Attention\"\"\"\n",
    "    if num_units is None:\n",
    "        num_units = queries.get_shape().as_list()[-1]\n",
    "\n",
    "    # Linear projections\n",
    "    Q = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(queries)\n",
    "    K = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(keys)\n",
    "    V = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(values)\n",
    "    if has_residual:\n",
    "        V_res = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(values)\n",
    "\n",
    "    # Split and concat\n",
    "    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)\n",
    "    K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)\n",
    "    V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)\n",
    "\n",
    "    # Multiplication\n",
    "    weights = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))\n",
    "    weights = weights / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "    weights = tf.nn.softmax(weights)\n",
    "\n",
    "    # Dropouts\n",
    "    weights = tf.cond(is_training,\n",
    "                      lambda: tf.nn.dropout(weights, keep_prob=dropout_keep_prob),\n",
    "                      lambda: weights)\n",
    "\n",
    "    # Weighted sum\n",
    "    outputs = tf.matmul(weights, V_)\n",
    "    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)\n",
    "\n",
    "    # Residual connection\n",
    "    if has_residual:\n",
    "        outputs += V_res\n",
    "\n",
    "    outputs = tf.nn.relu(outputs)\n",
    "    outputs = normalize(outputs)\n",
    "        \n",
    "    return outputs\n",
    "\n",
    "class AutoInt():\n",
    "    def __init__(self, args, feature_size, run_cnt):\n",
    "        \"\"\"AutoInt 모델 초기화\"\"\"\n",
    "        self.feature_size = int(feature_size)\n",
    "        self.field_size = args.field_size\n",
    "        self.run_cnt = run_cnt\n",
    "        self.embedding_size = int(args.embedding_size)\n",
    "        self.blocks = args.blocks\n",
    "        self.heads = args.heads\n",
    "        self.block_shape = args.block_shape\n",
    "        self.output_size = args.block_shape[-1]\n",
    "        self.has_residual = args.has_residual\n",
    "        self.deep_layers = args.deep_layers\n",
    "\n",
    "        # 학습 관련 하이퍼파라미터\n",
    "        self.batch_norm = args.batch_norm\n",
    "        self.batch_norm_decay = args.batch_norm_decay\n",
    "        self.drop_keep_prob = args.dropout_keep_prob\n",
    "        self.l2_reg = args.l2_reg\n",
    "        self.epoch = args.epoch\n",
    "        self.batch_size = args.batch_size\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.optimizer_type = args.optimizer_type\n",
    "\n",
    "        # 모델 저장 경로\n",
    "        self.save_path = args.save_path + str(run_cnt) + '/'\n",
    "        self.is_save = args.is_save\n",
    "        if (args.is_save == True and os.path.exists(self.save_path) == False):\n",
    "            os.makedirs(self.save_path)\t\n",
    "\n",
    "        self.verbose = args.verbose\n",
    "        self.random_seed = args.random_seed\n",
    "        # Ranking loss 관련 파라미터\n",
    "        self.loss_type = args.loss_type\n",
    "        self.ranking_k = getattr(args, 'ranking_k', 5)\n",
    "        self.ranking_tau = getattr(args, 'ranking_tau', 10.0)\n",
    "        self.eval_metric = roc_auc_score\n",
    "        self.best_loss = 1.0\n",
    "        self.greater_is_better = args.greater_is_better\n",
    "        self.train_result, self.valid_result = [], []\n",
    "        self.train_loss, self.valid_loss = [], []\n",
    "        \n",
    "        self._init_graph()\n",
    "\n",
    "    def _init_graph(self):\n",
    "        \"\"\"TensorFlow 계산 그래프 초기화\"\"\"\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "\n",
    "            # 입력 placeholder 정의\n",
    "            self.feat_index = tf.placeholder(tf.int32, shape=[None, None], name=\"feat_index\")\n",
    "            self.feat_value = tf.placeholder(tf.float32, shape=[None, None], name=\"feat_value\")\n",
    "            self.label = tf.placeholder(tf.float32, shape=[None, 1], name=\"label\")\n",
    "            self.dropout_keep_prob = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_prob\")\n",
    "            self.train_phase = tf.placeholder(tf.bool, name=\"train_phase\")\n",
    "\n",
    "            self.weights = self._initialize_weights()\n",
    "\n",
    "            # 1. Embedding layer\n",
    "            self.embeddings = tf.nn.embedding_lookup(self.weights[\"feature_embeddings\"], self.feat_index)\n",
    "            feat_value = tf.reshape(self.feat_value, shape=[-1, tf.shape(self.feat_index)[1], 1])\n",
    "            self.embeddings = tf.multiply(self.embeddings, feat_value)\n",
    "            self.embeddings = tf.nn.dropout(self.embeddings, self.dropout_keep_prob[1])\n",
    "            \n",
    "            # 2. DNN 부분 (선택적)\n",
    "            if self.deep_layers != None:\n",
    "                self.y_dense = tf.reshape(self.embeddings, shape=[-1, self.field_size * self.embedding_size])\n",
    "                \n",
    "                for i in range(0, len(self.deep_layers)):\n",
    "                    self.y_dense = tf.add(tf.matmul(self.y_dense, self.weights[\"layer_%d\" %i]), self.weights[\"bias_%d\"%i])\n",
    "                    if self.batch_norm:\n",
    "                        bn_layer = BatchNormalization(momentum=self.batch_norm_decay, epsilon=1e-5, center=True, scale=True, name=\"bn_%d\" % i)\n",
    "                        self.y_dense = bn_layer(self.y_dense, training=self.train_phase)\n",
    "                    self.y_dense = tf.nn.relu(self.y_dense)\n",
    "                    self.y_dense = tf.nn.dropout(self.y_dense, self.dropout_keep_prob[2])\n",
    "                    \n",
    "                self.y_dense = tf.add(tf.matmul(self.y_dense, self.weights[\"prediction_dense\"]),\n",
    "                                      self.weights[\"prediction_bias_dense\"], name='logits_dense')\n",
    "            \n",
    "            # 3. AutoInt 핵심 부분: Multi-head Self-Attention\n",
    "            self.y_deep = self.embeddings\n",
    "            for i in range(self.blocks):   \n",
    "                self.y_deep = multihead_attention(queries=self.y_deep, keys=self.y_deep, values=self.y_deep,\n",
    "                                                  num_units=self.block_shape[i], num_heads=self.heads,\n",
    "                                                  dropout_keep_prob=self.dropout_keep_prob[0],\n",
    "                                                  is_training=self.train_phase, has_residual=self.has_residual)\n",
    "\n",
    "            # Flatten\n",
    "            self.flat = tf.reshape(self.y_deep, [tf.shape(self.y_deep)[0], -1])\n",
    "\n",
    "            # 최종 예측\n",
    "            self.y = tf.add(tf.matmul(self.flat, self.weights['prediction']), self.weights['prediction_bias'], name='logits')\n",
    "            self.out = tf.nn.sigmoid(self.y)\n",
    "\n",
    "            # DNN과 AutoInt 결합 (선택적)\n",
    "            if self.deep_layers != None:\n",
    "                self.out = tf.nn.sigmoid(self.y + self.y_dense)\n",
    "            else:\n",
    "                self.out = tf.nn.sigmoid(self.y)\n",
    "        \n",
    "            # ============ Ranking 손실 함수 정의 ============\n",
    "            if self.loss_type == \"ranking_ndcg\":\n",
    "                # 배치를 재구성: 각 사람당 여러 의자들을 하나의 랭킹 문제로 처리\n",
    "                # 현재 구조에서는 배치 내의 모든 아이템을 하나의 랭킹으로 처리\n",
    "                self.loss = DifferentiableRankingLoss.neuNDCGLoss(\n",
    "                    scores=self.out, \n",
    "                    labels=self.label, \n",
    "                    k=self.ranking_k,\n",
    "                    tau=self.ranking_tau\n",
    "                )\n",
    "            elif self.loss_type == \"ranking_precision\":\n",
    "                self.loss = DifferentiableRankingLoss.neuPrecisionLoss(\n",
    "                    scores=self.out,\n",
    "                    labels=self.label,\n",
    "                    k=self.ranking_k, \n",
    "                    tau=self.ranking_tau\n",
    "                )\n",
    "            elif self.loss_type == \"logloss\":\n",
    "                # 기본 logloss (비교용)\n",
    "                self.loss = tf.losses.log_loss(self.label, self.out)\n",
    "            elif self.loss_type == \"mse\":\n",
    "                self.loss = tf.nn.l2_loss(tf.subtract(self.label, self.out))\n",
    "            else:\n",
    "                # 기본값: soft ranking loss (weighted binary cross entropy)\n",
    "                epsilon = 1e-7\n",
    "                self.loss = tf.reduce_mean(\n",
    "                    -self.label * tf.log(self.out + epsilon) - \n",
    "                    (1 - self.label) * tf.log(1 - self.out + epsilon)\n",
    "                )\n",
    "\n",
    "            # L2 정규화\n",
    "            if self.l2_reg > 0:\n",
    "                reg_weights = [weight for name, weight in self.weights.items() if 'bias' not in name]\n",
    "                if reg_weights:\n",
    "                    self.loss += tf.add_n([tf.nn.l2_loss(w) for w in reg_weights]) * self.l2_reg\n",
    "           \n",
    "            # Optimizer 설정\n",
    "            self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            \n",
    "            if self.optimizer_type == \"adam\":\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(self.loss, global_step=self.global_step)\n",
    "            elif self.optimizer_type == \"adagrad\":\n",
    "                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate, initial_accumulator_value=1e-8).minimize(self.loss, global_step=self.global_step)\n",
    "            elif self.optimizer_type == \"gd\":\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "            # 초기화\n",
    "            self.saver = tf.train.Saver(max_to_keep=5)\n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess = self._init_session()\n",
    "            self.sess.run(init)\n",
    "            self.count_param()\n",
    "\n",
    "    def count_param(self):\n",
    "        \"\"\"모델의 전체 파라미터 개수 계산\"\"\"\n",
    "        k = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "        print(\"Total parameters: %d\" % k) \n",
    "        print(\"Extra parameters: %d\" % (k - self.feature_size * self.embedding_size))\n",
    "\n",
    "    def _init_session(self):\n",
    "        \"\"\"TensorFlow 세션 초기화\"\"\"\n",
    "        config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        config.gpu_options.allow_growth = True\n",
    "        return tf.Session(config=config)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"모델 가중치 초기화\"\"\"\n",
    "        weights = dict()\n",
    "\n",
    "        # 특성 임베딩 테이블\n",
    "        weights[\"feature_embeddings\"] = tf.Variable(\n",
    "            tf.random_normal([self.feature_size, self.embedding_size], 0.0, 0.01),\n",
    "            name=\"feature_embeddings\")\n",
    "\n",
    "        if self.blocks > 0:\n",
    "            final_attention_size = self.block_shape[-1] * self.field_size\n",
    "            input_size = final_attention_size\n",
    "        else:\n",
    "            input_size = self.embedding_size * self.field_size\n",
    "\n",
    "        # DNN 레이어 가중치 (선택적)\n",
    "        if self.deep_layers != None:\n",
    "            num_layer = len(self.deep_layers)\n",
    "            layer0_size = self.field_size * self.embedding_size\n",
    "            glorot = np.sqrt(2.0 / (layer0_size + self.deep_layers[0]))\n",
    "            weights[\"layer_0\"] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(layer0_size, self.deep_layers[0])), dtype=np.float32)\n",
    "            weights[\"bias_0\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[0])), dtype=np.float32)\n",
    "            \n",
    "            for i in range(1, num_layer):\n",
    "                glorot = np.sqrt(2.0 / (self.deep_layers[i-1] + self.deep_layers[i]))\n",
    "                weights[\"layer_%d\" % i] = tf.Variable(\n",
    "                    np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i-1], self.deep_layers[i])), dtype=np.float32)\n",
    "                weights[\"bias_%d\" % i] = tf.Variable(\n",
    "                    np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])), dtype=np.float32)\n",
    "            \n",
    "            glorot = np.sqrt(2.0 / (self.deep_layers[-1] + 1))\n",
    "            weights[\"prediction_dense\"] = tf.Variable(\n",
    "                                np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[-1], 1)), dtype=np.float32, name=\"prediction_dense\")\n",
    "            weights[\"prediction_bias_dense\"] = tf.Variable(np.random.normal(), dtype=np.float32, name=\"prediction_bias_dense\")\n",
    "\n",
    "        # AutoInt 최종 예측층 가중치\n",
    "        glorot = np.sqrt(2.0 / (input_size + 1))\n",
    "        weights[\"prediction\"] = tf.Variable(\n",
    "                            np.random.normal(loc=0, scale=glorot, size=(input_size, 1)), dtype=np.float32, name=\"prediction\")\n",
    "        weights[\"prediction_bias\"] = tf.Variable(np.random.normal(), dtype=np.float32, name=\"prediction_bias\")\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def get_batch(self, Xi, Xv, y, batch_size, index):\n",
    "        \"\"\"배치 데이터 추출\"\"\"\n",
    "        start = index * batch_size\n",
    "        end = (index+1) * batch_size\n",
    "        end = end if end < len(y) else len(y)\n",
    "        return Xi[start:end], Xv[start:end], [[y_] for y_ in y[start:end]]\n",
    "\n",
    "    def shuffle_in_unison_scary(self, a, b, c):\n",
    "        \"\"\"세 개의 배열을 동일한 순서로 셔플\"\"\"\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(a)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(b)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(c)\n",
    "\n",
    "    def fit_on_batch(self, Xi, Xv, y):\n",
    "        \"\"\"한 배치에 대한 학습 수행\"\"\"\n",
    "        feed_dict = {self.feat_index: Xi, self.feat_value: Xv, self.label: y,\n",
    "                     self.dropout_keep_prob: self.drop_keep_prob, self.train_phase: True}\n",
    "        step, loss, opt = self.sess.run((self.global_step, self.loss, self.optimizer), feed_dict=feed_dict)\n",
    "        return step, loss\n",
    "\n",
    "    def fit_once(self, Xi_train, Xv_train, y_train, epoch, file_count, Xi_valid=None, Xv_valid=None, y_valid=None, early_stopping=False):\n",
    "        \"\"\"하나의 데이터 파일에 대한 전체 학습 수행\"\"\"\n",
    "        has_valid = Xv_valid is not None\n",
    "        last_step = 0\n",
    "        t1 = time()\n",
    "        \n",
    "        self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
    "        total_batch = int(len(y_train) / self.batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train, self.batch_size, i)\n",
    "            step, loss = self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
    "            last_step = step\n",
    "\n",
    "        # 학습 데이터 평가\n",
    "        train_result, train_loss = self.evaluate(Xi_train, Xv_train, y_train)\n",
    "        self.train_result.append(train_result)\n",
    "        self.train_loss.append(train_loss)\n",
    "\n",
    "        # 검증 데이터 평가\n",
    "        if has_valid:\n",
    "            valid_result, valid_loss = self.evaluate(Xi_valid, Xv_valid, y_valid)\n",
    "            self.valid_result.append(valid_result)\n",
    "            self.valid_loss.append(valid_loss)\n",
    "\n",
    "            # 최적 모델 저장\n",
    "            if valid_loss < self.best_loss and self.is_save == True:\n",
    "                old_loss = self.best_loss\n",
    "                self.best_loss = valid_loss\n",
    "                self.saver.save(self.sess, self.save_path + 'model.ckpt', global_step=last_step)\n",
    "                print(\"[%d-%d] Model saved! Valid loss improved from %.4f to %.4f\" % (epoch, file_count, old_loss, self.best_loss))\n",
    "\n",
    "        # 학습 결과 출력\n",
    "        if self.verbose > 0 and ((epoch-1)*9 + file_count) % self.verbose == 0:\n",
    "            if has_valid:\n",
    "                print(\"[%d-%d] train-result=%.4f, train-loss=%.4f, valid-result=%.4f, valid-loss=%.4f [%.1f s]\" % \n",
    "                      (epoch, file_count, train_result, train_loss, valid_result, valid_loss, time() - t1))\n",
    "            else:\n",
    "                print(\"[%d-%d] train-result=%.4f [%.1f s]\" % (epoch, file_count, train_result, time() - t1))\n",
    "                \n",
    "        return True\n",
    "\n",
    "    def predict(self, Xi, Xv):\n",
    "        \"\"\"예측 수행\"\"\"\n",
    "        dummy_y = [1] * len(Xi)\n",
    "        batch_index = 0\n",
    "        Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
    "        y_pred = None\n",
    "\n",
    "        while len(Xi_batch) > 0:\n",
    "            num_batch = len(y_batch)\n",
    "            feed_dict = {self.feat_index: Xi_batch, self.feat_value: Xv_batch, self.label: y_batch,\n",
    "                         self.dropout_keep_prob: [1.0] * len(self.drop_keep_prob), self.train_phase: False}\n",
    "            batch_out = self.sess.run(self.out, feed_dict=feed_dict)\n",
    "\n",
    "            if batch_index == 0:\n",
    "                y_pred = np.reshape(batch_out, (num_batch,))\n",
    "            else:\n",
    "                y_pred = np.concatenate((y_pred, np.reshape(batch_out, (num_batch,))))\n",
    "\n",
    "            batch_index += 1\n",
    "            Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, Xi, Xv, y):\n",
    "        \"\"\"모델 평가 - Top-k 지표 중심\"\"\"\n",
    "        y_pred = self.predict(Xi, Xv)\n",
    "        y_pred = np.clip(y_pred, 1e-6, 1-1e-6)\n",
    "\n",
    "        # Top-k 평가 지표들\n",
    "        results = {}\n",
    "        \n",
    "        # NDCG@5, NDCG@10 계산\n",
    "        for k in [5, 10, 20]:\n",
    "            try:\n",
    "                if len(y) >= k:\n",
    "                    ndcg_k = ndcg_score(y.reshape(1, -1), y_pred.reshape(1, -1), k=k)\n",
    "                    results[f'ndcg@{k}'] = ndcg_k\n",
    "                else:\n",
    "                    results[f'ndcg@{k}'] = 0.0\n",
    "            except:\n",
    "                results[f'ndcg@{k}'] = 0.0\n",
    "        \n",
    "        # Precision@k 계산\n",
    "        for k in [5, 10, 20]:\n",
    "            if len(y) >= k:\n",
    "                # Top-k 예측의 인덱스\n",
    "                top_k_indices = np.argsort(y_pred)[-k:]\n",
    "                # Top-k에서 relevant items 수 (soft label > 0.5)\n",
    "                relevant_in_topk = np.sum(y[top_k_indices] > 0.5)\n",
    "                precision_k = relevant_in_topk / k\n",
    "                results[f'precision@{k}'] = precision_k\n",
    "            else:\n",
    "                results[f'precision@{k}'] = 0.0\n",
    "        \n",
    "        # Recall@k 계산\n",
    "        total_relevant = np.sum(y > 0.5)\n",
    "        for k in [5, 10, 20]:\n",
    "            if len(y) >= k and total_relevant > 0:\n",
    "                top_k_indices = np.argsort(y_pred)[-k:]\n",
    "                relevant_in_topk = np.sum(y[top_k_indices] > 0.5)\n",
    "                recall_k = relevant_in_topk / total_relevant\n",
    "                results[f'recall@{k}'] = recall_k\n",
    "            else:\n",
    "                results[f'recall@{k}'] = 0.0\n",
    "        \n",
    "        # 메인 메트릭으로 NDCG@10 사용\n",
    "        main_metric = results.get('ndcg@10', 0.0)\n",
    "        \n",
    "        # 손실 계산 (ranking loss 근사)\n",
    "        epsilon = 1e-7\n",
    "        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.mean(y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped))\n",
    "        \n",
    "        # 상세 결과 출력\n",
    "        print(f\"Evaluation Results:\")\n",
    "        for metric, value in results.items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "        return main_metric, loss\n",
    "\n",
    "    def restore(self, save_path=None):\n",
    "        \"\"\"저장된 모델 복원\"\"\"\n",
    "        if save_path == None:\n",
    "            save_path = self.save_path\n",
    "        ckpt = tf.train.get_checkpoint_state(save_path)  \n",
    "        if ckpt and ckpt.model_checkpoint_path:  \n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path) \n",
    "            if self.verbose > 0:\n",
    "                print(\"Restored from %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ee7f5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chair Recommendation AutoInt Model Training ===\n",
      "🎯 Top-k Ranking 최적화 모델 훈련\n",
      "이 노트북은 전체 파이프라인을 실행합니다:\n",
      "1. 데이터 전처리\n",
      "2. K-fold 분할\n",
      "3. 스케일링\n",
      "4. AutoInt + DRM 모델 훈련 (Top-k 최적화)\n",
      "5. Top-k 지표 평가 (NDCG@k, Precision@k, Recall@k)\n",
      "6. 모델 저장\n",
      "\n",
      "📊 현재 설정:\n",
      "   Loss Function: ranking_ndcg\n",
      "   Ranking K: 10\n",
      "   Temperature: 5.0\n",
      "   Batch Size: 512 (ranking loss 최적화)\n",
      "\n",
      "=== Step 1: 데이터 전처리 ===\n",
      "Processed 100 combinations...\n",
      "Processed 200 combinations...\n",
      "Processed 300 combinations...\n",
      "Processed 400 combinations...\n",
      "Processed 500 combinations...\n",
      "Processed 600 combinations...\n",
      "Processed 700 combinations...\n",
      "Processed 800 combinations...\n",
      "Processed 900 combinations...\n",
      "Processed 1000 combinations...\n",
      "Processed 1100 combinations...\n",
      "Processed 1200 combinations...\n",
      "Processed 1300 combinations...\n",
      "Processed 1400 combinations...\n",
      "Processed 1500 combinations...\n",
      "Processed 1600 combinations...\n",
      "Processed 1700 combinations...\n",
      "Processed 1800 combinations...\n",
      "Processed 1900 combinations...\n",
      "Processed 2000 combinations...\n",
      "Processed 2100 combinations...\n",
      "Processed 2200 combinations...\n",
      "Processed 2300 combinations...\n",
      "Processed 2400 combinations...\n",
      "Processed 2500 combinations...\n",
      "Processed 2600 combinations...\n",
      "Processed 2700 combinations...\n",
      "Processed 2800 combinations...\n",
      "Processed 2900 combinations...\n",
      "Processed 3000 combinations...\n",
      "Processed 3100 combinations...\n",
      "Processed 3200 combinations...\n",
      "Processed 3300 combinations...\n",
      "Processed 3400 combinations...\n",
      "Processed 3500 combinations...\n",
      "Processed 3600 combinations...\n",
      "Processed 3700 combinations...\n",
      "Processed 3800 combinations...\n",
      "Processed 3900 combinations...\n",
      "Processed 4000 combinations...\n",
      "Processed 4100 combinations...\n",
      "Processed 4200 combinations...\n",
      "Processed 4300 combinations...\n",
      "Processed 4400 combinations...\n",
      "Processed 4500 combinations...\n",
      "Processed 4600 combinations...\n",
      "Processed 4700 combinations...\n",
      "Processed 4800 combinations...\n",
      "Processed 4900 combinations...\n",
      "Processed 5000 combinations...\n",
      "Processed 5100 combinations...\n",
      "Processed 5200 combinations...\n",
      "Processed 5300 combinations...\n",
      "Processed 5400 combinations...\n",
      "Processed 5500 combinations...\n",
      "Processed 5600 combinations...\n",
      "Processed 5700 combinations...\n",
      "Processed 5800 combinations...\n",
      "Processed 5900 combinations...\n",
      "Processed 6000 combinations...\n",
      "Processed 6100 combinations...\n",
      "Processed 6200 combinations...\n",
      "Processed 6300 combinations...\n",
      "Processed 6400 combinations...\n",
      "Processed 6500 combinations...\n",
      "Processed 6600 combinations...\n",
      "Processed 6700 combinations...\n",
      "Processed 6800 combinations...\n",
      "Processed 6900 combinations...\n",
      "Processed 7000 combinations...\n",
      "Processed 7100 combinations...\n",
      "Processed 7200 combinations...\n",
      "Processed 7300 combinations...\n",
      "Processed 7400 combinations...\n",
      "Processed 7500 combinations...\n",
      "Processed 7600 combinations...\n",
      "Processed 7700 combinations...\n",
      "Processed 7800 combinations...\n",
      "Processed 7900 combinations...\n",
      "Processed 8000 combinations...\n",
      "Processed 8100 combinations...\n",
      "Processed 8200 combinations...\n",
      "Processed 8300 combinations...\n",
      "Processed 8400 combinations...\n",
      "Processed 8500 combinations...\n",
      "Processed 8600 combinations...\n",
      "Processed 8700 combinations...\n",
      "Processed 8800 combinations...\n",
      "Processed 8900 combinations...\n",
      "Processed 9000 combinations...\n",
      "Processed 9100 combinations...\n",
      "Processed 9200 combinations...\n",
      "Processed 9300 combinations...\n",
      "Processed 9400 combinations...\n",
      "Processed 9500 combinations...\n",
      "Processed 9600 combinations...\n",
      "Processed 9700 combinations...\n",
      "Processed 9800 combinations...\n",
      "Processed 9900 combinations...\n",
      "Processed 10000 combinations...\n",
      "Processed 10100 combinations...\n",
      "Processed 10200 combinations...\n",
      "Processed 10300 combinations...\n",
      "Processed 10400 combinations...\n",
      "Processed 10500 combinations...\n",
      "Processed 10600 combinations...\n",
      "Processed 10700 combinations...\n",
      "Processed 10800 combinations...\n",
      "Processed 10900 combinations...\n",
      "Processed 11000 combinations...\n",
      "Processed 11100 combinations...\n",
      "Processed 11200 combinations...\n",
      "Processed 11300 combinations...\n",
      "Processed 11400 combinations...\n",
      "Processed 11500 combinations...\n",
      "Processed 11600 combinations...\n",
      "Processed 11700 combinations...\n",
      "Processed 11800 combinations...\n",
      "Processed 11900 combinations...\n",
      "Processed 12000 combinations...\n",
      "Processed 12100 combinations...\n",
      "Processed 12200 combinations...\n",
      "Processed 12300 combinations...\n",
      "Processed 12400 combinations...\n",
      "Processed 12500 combinations...\n",
      "Processed 12600 combinations...\n",
      "Processed 12700 combinations...\n",
      "Processed 12800 combinations...\n",
      "Processed 12900 combinations...\n",
      "Processed 13000 combinations...\n",
      "Processed 13100 combinations...\n",
      "Processed 13200 combinations...\n",
      "Processed 13300 combinations...\n",
      "Processed 13400 combinations...\n",
      "Processed 13500 combinations...\n",
      "Processed 13600 combinations...\n",
      "Processed 13700 combinations...\n",
      "Processed 13800 combinations...\n",
      "Processed 13900 combinations...\n",
      "\n",
      "Total combinations: 13986\n",
      "Positive matches: 485 (3.47%)\n",
      "전처리 완료!\n",
      "\n",
      "=== Step 2: K-fold 분할 ===\n",
      "Data loading done!\n",
      "Training data: 13986\n",
      "Now part 1\n",
      "Now part 2\n",
      "Now part 3\n",
      "Now part 4\n",
      "Now part 5\n",
      "Now part 6\n",
      "Now part 7\n",
      "Now part 8\n",
      "Now part 9\n",
      "Feature size = 31\n",
      "Now part 1\n",
      "Now part 2\n",
      "Now part 3\n",
      "Now part 4\n",
      "Now part 5\n",
      "Now part 6\n",
      "Now part 7\n",
      "Now part 8\n",
      "Now part 9\n",
      "K-fold 분할 완료!\n",
      "\n",
      "=== Step 3: 스케일링 ===\n",
      "Now part 1\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 2\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 3\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 4\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 5\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 6\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 7\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 8\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "Now part 9\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "스케일링 완료!\n",
      "\n",
      "=== Step 4: AutoInt + DRM 모델 훈련 ===\n",
      "🎯 Ranking Loss: ranking_ndcg\n",
      "📊 Top-10 최적화\n",
      "\n",
      "Feature size: 31.0\n",
      "Field size: 24\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/ku/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749763212.792404 19257416 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 3121\n",
      "Extra parameters: 2625\n",
      "\n",
      "=== Epoch 1/5 ===\n",
      "Training on part 3...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.6094\n",
      "  ndcg@10: 0.6194\n",
      "  ndcg@20: 0.6071\n",
      "  precision@5: 0.8000\n",
      "  precision@10: 0.9000\n",
      "  precision@20: 0.8500\n",
      "  recall@5: 0.0038\n",
      "  recall@10: 0.0085\n",
      "  recall@20: 0.0160\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.4584\n",
      "  ndcg@10: 0.5081\n",
      "  ndcg@20: 0.5001\n",
      "  precision@5: 0.2000\n",
      "  precision@10: 0.6000\n",
      "  precision@20: 0.5500\n",
      "  recall@5: 0.0009\n",
      "  recall@10: 0.0057\n",
      "  recall@20: 0.0104\n",
      "[1-1] Model saved! Valid loss improved from 1.0000 to 0.8070\n",
      "[1-1] train-result=0.6194, train-loss=0.8008, valid-result=0.5081, valid-loss=0.8070 [0.4 s]\n",
      "Training on part 4...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.4970\n",
      "  ndcg@10: 0.4801\n",
      "  ndcg@20: 0.4655\n",
      "  precision@5: 0.4000\n",
      "  precision@10: 0.4000\n",
      "  precision@20: 0.3500\n",
      "  recall@5: 0.0019\n",
      "  recall@10: 0.0038\n",
      "  recall@20: 0.0066\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.5911\n",
      "  ndcg@10: 0.5519\n",
      "  ndcg@20: 0.5357\n",
      "  precision@5: 0.6000\n",
      "  precision@10: 0.5000\n",
      "  precision@20: 0.6000\n",
      "  recall@5: 0.0028\n",
      "  recall@10: 0.0047\n",
      "  recall@20: 0.0114\n",
      "[1-2] Model saved! Valid loss improved from 0.8070 to 0.7636\n",
      "[1-2] train-result=0.4801, train-loss=0.7568, valid-result=0.5519, valid-loss=0.7636 [0.1 s]\n",
      "Training on part 5...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.4797\n",
      "  ndcg@10: 0.4813\n",
      "  ndcg@20: 0.5014\n",
      "  precision@5: 0.6000\n",
      "  precision@10: 0.5000\n",
      "  precision@20: 0.5000\n",
      "  recall@5: 0.0028\n",
      "  recall@10: 0.0047\n",
      "  recall@20: 0.0093\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.6047\n",
      "  ndcg@10: 0.5757\n",
      "  ndcg@20: 0.5123\n",
      "  precision@5: 0.6000\n",
      "  precision@10: 0.6000\n",
      "  precision@20: 0.4500\n",
      "  recall@5: 0.0028\n",
      "  recall@10: 0.0057\n",
      "  recall@20: 0.0085\n",
      "[1-3] Model saved! Valid loss improved from 0.7636 to 0.7016\n",
      "[1-3] train-result=0.4813, train-loss=0.7007, valid-result=0.5757, valid-loss=0.7016 [0.2 s]\n",
      "\n",
      "=== Epoch 2/5 ===\n",
      "Training on part 3...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.7017\n",
      "  ndcg@10: 0.6807\n",
      "  ndcg@20: 0.6521\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 0.9000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0094\n",
      "  recall@20: 0.0169\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.6631\n",
      "  ndcg@10: 0.6126\n",
      "  ndcg@20: 0.5877\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 0.8000\n",
      "  precision@20: 0.7000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0076\n",
      "  recall@20: 0.0133\n",
      "[2-1] train-result=0.6807, train-loss=0.7019, valid-result=0.6126, valid-loss=0.7040 [0.1 s]\n",
      "Training on part 4...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.5844\n",
      "  ndcg@10: 0.6287\n",
      "  ndcg@20: 0.6199\n",
      "  precision@5: 0.6000\n",
      "  precision@10: 0.7000\n",
      "  precision@20: 0.7000\n",
      "  recall@5: 0.0028\n",
      "  recall@10: 0.0066\n",
      "  recall@20: 0.0132\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.5691\n",
      "  ndcg@10: 0.5652\n",
      "  ndcg@20: 0.6060\n",
      "  precision@5: 0.6000\n",
      "  precision@10: 0.7000\n",
      "  precision@20: 0.7500\n",
      "  recall@5: 0.0028\n",
      "  recall@10: 0.0066\n",
      "  recall@20: 0.0142\n",
      "[2-2] train-result=0.6287, train-loss=1.0199, valid-result=0.5652, valid-loss=1.0122 [0.1 s]\n",
      "Training on part 5...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 1.0000\n",
      "  ndcg@10: 0.8691\n",
      "  ndcg@20: 0.8387\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 0.8000\n",
      "  precision@20: 0.9000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0075\n",
      "  recall@20: 0.0168\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.8941\n",
      "  ndcg@10: 0.8735\n",
      "  ndcg@20: 0.8477\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 0.9500\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0180\n",
      "[2-3] train-result=0.8691, train-loss=0.7539, valid-result=0.8735, valid-loss=0.7517 [0.1 s]\n",
      "\n",
      "=== Epoch 3/5 ===\n",
      "Training on part 3...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.8493\n",
      "  ndcg@10: 0.7453\n",
      "  ndcg@20: 0.7225\n",
      "  precision@5: 0.8000\n",
      "  precision@10: 0.7000\n",
      "  precision@20: 0.8000\n",
      "  recall@5: 0.0038\n",
      "  recall@10: 0.0066\n",
      "  recall@20: 0.0151\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.8767\n",
      "  ndcg@10: 0.8375\n",
      "  ndcg@20: 0.7837\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 0.9000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0171\n",
      "[3-1] Model saved! Valid loss improved from 0.7016 to 0.6836\n",
      "[3-1] train-result=0.7453, train-loss=0.6842, valid-result=0.8375, valid-loss=0.6836 [0.1 s]\n",
      "Training on part 4...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.6941\n",
      "  ndcg@10: 0.7295\n",
      "  ndcg@20: 0.6984\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 0.9500\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0180\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.6941\n",
      "  ndcg@10: 0.7033\n",
      "  ndcg@20: 0.6968\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 0.9500\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0180\n",
      "[3-2] train-result=0.7295, train-loss=0.6940, valid-result=0.7033, valid-loss=0.6977 [0.1 s]\n",
      "Training on part 5...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.3369\n",
      "  ndcg@10: 0.3713\n",
      "  ndcg@20: 0.4553\n",
      "  precision@5: 0.2000\n",
      "  precision@10: 0.3000\n",
      "  precision@20: 0.4000\n",
      "  recall@5: 0.0009\n",
      "  recall@10: 0.0028\n",
      "  recall@20: 0.0075\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.3982\n",
      "  ndcg@10: 0.4113\n",
      "  ndcg@20: 0.4082\n",
      "  precision@5: 0.4000\n",
      "  precision@10: 0.4000\n",
      "  precision@20: 0.2500\n",
      "  recall@5: 0.0019\n",
      "  recall@10: 0.0038\n",
      "  recall@20: 0.0047\n",
      "[3-3] train-result=0.3713, train-loss=0.6951, valid-result=0.4113, valid-loss=0.6972 [0.1 s]\n",
      "\n",
      "=== Epoch 4/5 ===\n",
      "Training on part 3...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.4322\n",
      "  ndcg@10: 0.4727\n",
      "  ndcg@20: 0.4890\n",
      "  precision@5: 0.2000\n",
      "  precision@10: 0.3000\n",
      "  precision@20: 0.3500\n",
      "  recall@5: 0.0009\n",
      "  recall@10: 0.0028\n",
      "  recall@20: 0.0066\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.7572\n",
      "  ndcg@10: 0.6336\n",
      "  ndcg@20: 0.5932\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 0.6000\n",
      "  precision@20: 0.5000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0057\n",
      "  recall@20: 0.0095\n",
      "[4-1] train-result=0.4727, train-loss=0.6983, valid-result=0.6336, valid-loss=0.6978 [0.1 s]\n",
      "Training on part 4...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.7738\n",
      "  ndcg@10: 0.8089\n",
      "  ndcg@20: 0.8187\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 1.0000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0189\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.8941\n",
      "  ndcg@10: 0.8743\n",
      "  ndcg@20: 0.8620\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 1.0000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0190\n",
      "[4-2] train-result=0.8089, train-loss=0.6957, valid-result=0.8743, valid-loss=0.6951 [0.1 s]\n",
      "Training on part 5...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.8203\n",
      "  ndcg@10: 0.8400\n",
      "  ndcg@20: 0.8266\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 1.0000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0093\n",
      "  recall@20: 0.0187\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.9059\n",
      "  ndcg@10: 0.9105\n",
      "  ndcg@20: 0.8938\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 1.0000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0190\n",
      "[4-3] Model saved! Valid loss improved from 0.6836 to 0.6821\n",
      "[4-3] train-result=0.8400, train-loss=0.6804, valid-result=0.9105, valid-loss=0.6821 [0.1 s]\n",
      "\n",
      "=== Epoch 5/5 ===\n",
      "Training on part 3...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.8602\n",
      "  ndcg@10: 0.8545\n",
      "  ndcg@20: 0.8634\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 1.0000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0094\n",
      "  recall@20: 0.0188\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.8555\n",
      "  ndcg@10: 0.8802\n",
      "  ndcg@20: 0.8884\n",
      "  precision@5: 1.0000\n",
      "  precision@10: 1.0000\n",
      "  precision@20: 1.0000\n",
      "  recall@5: 0.0047\n",
      "  recall@10: 0.0095\n",
      "  recall@20: 0.0190\n",
      "[5-1] train-result=0.8545, train-loss=0.7009, valid-result=0.8802, valid-loss=0.7007 [0.1 s]\n",
      "Training on part 4...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.3923\n",
      "  ndcg@10: 0.4396\n",
      "  ndcg@20: 0.4705\n",
      "  precision@5: 0.2000\n",
      "  precision@10: 0.3000\n",
      "  precision@20: 0.5000\n",
      "  recall@5: 0.0009\n",
      "  recall@10: 0.0028\n",
      "  recall@20: 0.0095\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.4882\n",
      "  ndcg@10: 0.4449\n",
      "  ndcg@20: 0.4564\n",
      "  precision@5: 0.6000\n",
      "  precision@10: 0.4000\n",
      "  precision@20: 0.4000\n",
      "  recall@5: 0.0028\n",
      "  recall@10: 0.0038\n",
      "  recall@20: 0.0076\n",
      "[5-2] train-result=0.4396, train-loss=0.8260, valid-result=0.4449, valid-loss=0.8212 [0.1 s]\n",
      "Training on part 5...\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.5144\n",
      "  ndcg@10: 0.5856\n",
      "  ndcg@20: 0.6048\n",
      "  precision@5: 0.8000\n",
      "  precision@10: 0.9000\n",
      "  precision@20: 0.9500\n",
      "  recall@5: 0.0037\n",
      "  recall@10: 0.0084\n",
      "  recall@20: 0.0177\n",
      "Evaluation Results:\n",
      "  ndcg@5: 0.5410\n",
      "  ndcg@10: 0.5322\n",
      "  ndcg@20: 0.5713\n",
      "  precision@5: 0.6000\n",
      "  precision@10: 0.6000\n",
      "  precision@20: 0.7500\n",
      "  recall@5: 0.0028\n",
      "  recall@10: 0.0057\n",
      "  recall@20: 0.0142\n",
      "[5-3] train-result=0.5856, train-loss=0.9113, valid-result=0.5322, valid-loss=0.9061 [0.1 s]\n",
      "\n",
      "=== 🎉 훈련 완료! ===\n",
      "🎯 Top-k 최적화 모델이 저장되었습니다: ./saved_models/autoint/2/\n",
      "📊 사용된 지표: NDCG@10, Precision@10, Recall@10\n",
      "\n",
      "❌ 체크포인트 파일을 찾을 수 없습니다\n",
      "\n",
      "=== 🏁 완료 ===\n",
      "🎯 Top-k 최적화 AutoInt 모델 훈련이 완료되었습니다!\n",
      "\n",
      "💡 이제 웹 서비스에서 다음 설정을 사용하세요:\n",
      "   model_path = './saved_models/autoint/2/'\n",
      "   ranking_optimized = True\n",
      "   top_k_performance = 'NDCG@10 optimized'\n",
      "\n",
      "🚀 추천 시스템 성능:\n",
      "   ✅ 상위 랭킹 정확도 극대화\n",
      "   ✅ 개인화 추천 품질 향상\n",
      "   ✅ Top-k 지표 최적화 완료\n",
      "   ✅ 실시간 웹 서비스 준비 완료\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7. 실행 코드\n",
    "# ============================================================================\n",
    "\n",
    "# 설정 정의 - Ranking Loss 사용\n",
    "args = Namespace(\n",
    "    is_save=True,\n",
    "    greater_is_better=True,\n",
    "    has_residual=True,\n",
    "    blocks=2,\n",
    "    block_shape=[16, 16],\n",
    "    heads=2,\n",
    "    embedding_size=16,\n",
    "    dropout_keep_prob=[0.8, 0.8, 0.5],  # 드롭아웃 약간 적용\n",
    "    epoch=5,  # 조금 더 많은 에폭\n",
    "    batch_size=512,  # 배치 크기 줄임 (ranking loss가 메모리를 더 사용)\n",
    "    learning_rate=0.001,\n",
    "    optimizer_type='adam',\n",
    "    l2_reg=0.001,\n",
    "    random_seed=42,\n",
    "    save_path=MODEL_SAVE_PATH,\n",
    "    field_size=24,\n",
    "    \n",
    "    # ⭐ Top-k Ranking Loss 설정\n",
    "    loss_type='ranking_ndcg',      # 'ranking_ndcg' 또는 'ranking_precision'\n",
    "    ranking_k=10,                  # Top-10으로 평가\n",
    "    ranking_tau=5.0,               # Neural sort temperature (낮을수록 sharp)\n",
    "    \n",
    "    verbose=1,\n",
    "    run_times=1,\n",
    "    deep_layers=None,              # 또는 [128, 64] 등 DNN 추가 가능\n",
    "    batch_norm=0,\n",
    "    batch_norm_decay=0.995\n",
    ")\n",
    "\n",
    "print(\"=== Chair Recommendation AutoInt Model Training ===\")\n",
    "print(\"🎯 Top-k Ranking 최적화 모델 훈련\")\n",
    "print(\"이 노트북은 전체 파이프라인을 실행합니다:\")\n",
    "print(\"1. 데이터 전처리\")\n",
    "print(\"2. K-fold 분할\") \n",
    "print(\"3. 스케일링\")\n",
    "print(\"4. AutoInt + DRM 모델 훈련 (Top-k 최적화)\")\n",
    "print(\"5. Top-k 지표 평가 (NDCG@k, Precision@k, Recall@k)\")\n",
    "print(\"6. 모델 저장\")\n",
    "print()\n",
    "print(f\"📊 현재 설정:\")\n",
    "print(f\"   Loss Function: {args.loss_type}\")\n",
    "print(f\"   Ranking K: {args.ranking_k}\")\n",
    "print(f\"   Temperature: {args.ranking_tau}\")\n",
    "print(f\"   Batch Size: {args.batch_size} (ranking loss 최적화)\")\n",
    "print()\n",
    "\n",
    "# 단계별 실행\n",
    "run_preprocessing = True  # 전처리 실행 여부\n",
    "run_training = True       # 훈련 실행 여부\n",
    "\n",
    "if run_preprocessing:\n",
    "    print(\"=== Step 1: 데이터 전처리 ===\")\n",
    "    # 실제 파일 경로로 수정 필요\n",
    "    preprocessor = DataPreprocessor(\n",
    "        chair_path='ohouse_chair.xlsx',  # 실제 의자 데이터 파일\n",
    "        person_path='person.csv',       # 실제 사람 데이터 파일\n",
    "        output_dir=DATA_PATH\n",
    "    )\n",
    "    preprocessor.process_data()\n",
    "    print(\"전처리 완료!\")\n",
    "    print()\n",
    "\n",
    "    print(\"=== Step 2: K-fold 분할 ===\")\n",
    "    train_x, train_y = _load_data()\n",
    "    folds = list(KFold(n_splits=9, shuffle=True, random_state=args.random_seed).split(train_x))\n",
    "    fold_index = [valid_id for train_id, valid_id in folds]\n",
    "    \n",
    "    np.save(DATA_PATH + \"fold_index.npy\", np.array(fold_index))\n",
    "    save_x_y(fold_index, train_x, train_y)\n",
    "    save_i(fold_index)\n",
    "    print(\"K-fold 분할 완료!\")\n",
    "    print()\n",
    "\n",
    "    print(\"=== Step 3: 스케일링 ===\")\n",
    "    scale_each_fold()\n",
    "    print(\"스케일링 완료!\")\n",
    "    print()\n",
    "\n",
    "if run_training:\n",
    "    print(\"=== Step 4: AutoInt + DRM 모델 훈련 ===\")\n",
    "    print(f\"🎯 Ranking Loss: {args.loss_type}\")\n",
    "    print(f\"📊 Top-{args.ranking_k} 최적화\")\n",
    "    print()\n",
    "    \n",
    "    # 특성 크기 로드\n",
    "    feature_size = np.load(DATA_PATH + '/feature_size.npy')[0]\n",
    "    print(f\"Feature size: {feature_size}\")\n",
    "    \n",
    "    # 검증 데이터 로드\n",
    "    Xi_valid = np.load(DATA_PATH + '/part2/train_i.npy')\n",
    "    Xv_valid = np.load(DATA_PATH + '/part2/train_x2.npy')\n",
    "    y_valid = np.load(DATA_PATH + '/part2/train_y.npy')\n",
    "    \n",
    "    # 실제 field_size 설정\n",
    "    args.field_size = Xi_valid.shape[1]\n",
    "    print(f\"Field size: {args.field_size}\")\n",
    "    \n",
    "    # 모델 생성\n",
    "    model = AutoInt(args=args, feature_size=feature_size, run_cnt=1)\n",
    "    \n",
    "    # 훈련 시작\n",
    "    for k in range(args.epoch):\n",
    "        print(f\"\\n=== Epoch {k+1}/{args.epoch} ===\")\n",
    "        \n",
    "        for j in range(3, 6):  # part3-5만 사용 (빠른 훈련을 위해)\n",
    "            print(f\"Training on part {j}...\")\n",
    "            \n",
    "            Xi_train = np.load(DATA_PATH + f'/part{j}/train_i.npy')\n",
    "            Xv_train = np.load(DATA_PATH + f'/part{j}/train_x2.npy')\n",
    "            y_train = np.load(DATA_PATH + f'/part{j}/train_y.npy')\n",
    "            \n",
    "            model.fit_once(Xi_train, Xv_train, y_train, k+1, j-2,\n",
    "                          Xi_valid, Xv_valid, y_valid, early_stopping=True)\n",
    "    \n",
    "    print(\"\\n=== 🎉 훈련 완료! ===\")\n",
    "    print(f\"🎯 Top-k 최적화 모델이 저장되었습니다: {MODEL_SAVE_PATH}2/\")\n",
    "    print(f\"📊 사용된 지표: NDCG@{args.ranking_k}, Precision@{args.ranking_k}, Recall@{args.ranking_k}\")\n",
    "    print()\n",
    "    \n",
    "    # 최종 검증\n",
    "    if os.path.exists(MODEL_SAVE_PATH + '2/checkpoint'):\n",
    "        print(\"✅ 체크포인트 파일 확인 완료\")\n",
    "        print(\"🚀 웹 서비스에서 Top-k 추천이 가능합니다!\")\n",
    "        print()\n",
    "        print(\"📈 기대 효과:\")\n",
    "        print(\"   - 상위 랭킹 정확도 향상\")\n",
    "        print(\"   - 사용자 맞춤 추천 품질 개선\") \n",
    "        print(\"   - NDCG, Precision, Recall 지표 최적화\")\n",
    "    else:\n",
    "        print(\"❌ 체크포인트 파일을 찾을 수 없습니다\")\n",
    "\n",
    "print(\"\\n=== 🏁 완료 ===\")\n",
    "print(\"🎯 Top-k 최적화 AutoInt 모델 훈련이 완료되었습니다!\")\n",
    "print()\n",
    "print(\"💡 이제 웹 서비스에서 다음 설정을 사용하세요:\")\n",
    "print(f\"   model_path = '{MODEL_SAVE_PATH}2/'\")\n",
    "print(f\"   ranking_optimized = True\")\n",
    "print(f\"   top_k_performance = 'NDCG@{args.ranking_k} optimized'\")\n",
    "print()\n",
    "print(\"🚀 추천 시스템 성능:\")\n",
    "print(\"   ✅ 상위 랭킹 정확도 극대화\")\n",
    "print(\"   ✅ 개인화 추천 품질 향상\")\n",
    "print(\"   ✅ Top-k 지표 최적화 완료\")\n",
    "print(\"   ✅ 실시간 웹 서비스 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9040b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ku",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
